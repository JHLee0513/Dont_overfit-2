{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 5 - Synthetic Data\n",
    "\n",
    "Now that we have seen that the distribution between the training dataset and test dataset are quite similar, and that normal CV works, let's now try to generate a synthetic dataset based on our training dataset to help generalize the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from boruta import BorutaPy\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold, cross_val_score, GridSearchCV, RepeatedStratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from eli5.sklearn import PermutationImportance\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "import featuretools as ft\n",
    "import json\n",
    "import ast\n",
    "import time\n",
    "from sklearn import linear_model\n",
    "import eli5\n",
    "import gc\n",
    "gc.enable()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"/Users/JoonH/dont-overfit-ii/train.csv\")\n",
    "test_df = pd.read_csv(\"/Users/JoonH/dont-overfit-ii/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_data(train_df, test_df):\n",
    "    x_train = train_df.drop(['target','id'],axis=1)\n",
    "    #x_train = np.expand_dims(x_train, axis = -1)\n",
    "    y_train = train_df['target']\n",
    "    x_test = test_df.drop(['id'],axis=1)\n",
    "\n",
    "    n_fold = 20\n",
    "    folds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=42)\n",
    "    #scaler = StandardScaler()\n",
    "    #x_train = scaler.fit_transform(x_train)\n",
    "    #x_test = scaler.transform(x_test)\n",
    "    x_train = np.array(x_train)\n",
    "    return x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_generative(G_in, dense_dim=200, out_dim=50, lr=1e-3):\n",
    "    x = Dense(dense_dim, activation = 'tanh') (G_in)\n",
    "    G_out = Dense(out_dim, activation='tanh') (x)\n",
    "    G = Model(G_in, G_out)\n",
    "    opt = Adam(lr=lr)\n",
    "    G.compile(loss='binary_crossentropy',optimizer=opt)\n",
    "    return G, G_out\n",
    "    \n",
    "G_in = Input(shape=[300])\n",
    "G,G_out = get_generative(G_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 50, 1)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 46, 50)            300       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 46, 50)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2300)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 50)                115050    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 115,452\n",
      "Trainable params: 115,452\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def get_discriminative(D_in, lr=1e-3, drate=.25, n_channels=50, conv_sz=5, leak=.2):\n",
    "    x = Reshape((-1, 1))(D_in)\n",
    "    x = Conv1D(n_channels, conv_sz, activation='relu')(x)\n",
    "    x = Dropout(drate)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(n_channels)(x)\n",
    "    D_out = Dense(2, activation='sigmoid')(x)\n",
    "    D = Model(D_in, D_out)\n",
    "    dopt = Adam(lr=lr)\n",
    "    D.compile(loss='binary_crossentropy', optimizer=dopt)\n",
    "    return D, D_out\n",
    "\n",
    "D_in = Input(shape=[50])\n",
    "D, D_out = get_discriminative(D_in)\n",
    "D.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "model_2 (Model)              (None, 50)                70250     \n",
      "_________________________________________________________________\n",
      "model_3 (Model)              (None, 2)                 115452    \n",
      "=================================================================\n",
      "Total params: 185,702\n",
      "Trainable params: 70,250\n",
      "Non-trainable params: 115,452\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def set_trainability(model, trainable=False):\n",
    "    model.trainable = trainable\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = trainable\n",
    "        \n",
    "def make_gan(GAN_in, G, D):\n",
    "    set_trainability(D, False)\n",
    "    x = G(GAN_in)\n",
    "    GAN_out = D(x)\n",
    "    GAN = Model(GAN_in, GAN_out)\n",
    "    GAN.compile(loss='binary_crossentropy', optimizer=G.optimizer)\n",
    "    return GAN, GAN_out\n",
    "\n",
    "GAN_in = Input([300])\n",
    "GAN, GAN_out = make_gan(GAN_in, G, D)\n",
    "GAN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-d051cf53d311>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mpretrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mD\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-24-d051cf53d311>\u001b[0m in \u001b[0;36mpretrain\u001b[1;34m(G, D, noise_dim, n_samples, batch_size)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpretrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mD\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnoise_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample_data_and_gen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnoise_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnoise_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mset_trainability\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mD\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-24-d051cf53d311>\u001b[0m in \u001b[0;36msample_data_and_gen\u001b[1;34m(G, noise_dim, n_samples)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mXN_noise\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnoise_dim\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mXN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXN_noise\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly"
     ]
    }
   ],
   "source": [
    "def sample_data_and_gen(G, noise_dim=300, n_samples=10000):\n",
    "    XT = sample_data(train_df,test_df)\n",
    "    XN_noise = np.random.uniform(0, 1, size=[n_samples, noise_dim])\n",
    "    XN = G.predict(XN_noise)\n",
    "    X = np.concatenate((XT, XN))\n",
    "    y = np.zeros((2*n_samples, 2))\n",
    "    y[:n_samples, 1] = 1\n",
    "    y[n_samples:, 0] = 1\n",
    "    return X, y\n",
    "\n",
    "def pretrain(G, D, noise_dim=300, n_samples=10000, batch_size=32):\n",
    "    X, y = sample_data_and_gen(G, n_samples=n_samples, noise_dim=noise_dim)\n",
    "    set_trainability(D, True)\n",
    "    D.fit(X, y, epochs=1, batch_size=batch_size)\n",
    "\n",
    "pretrain(G, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_noise(G, noise_dim=10, n_samples=10000):\n",
    "    X = np.random.uniform(0, 1, size=[n_samples, noise_dim])\n",
    "    y = np.zeros((n_samples, 2))\n",
    "    y[:, 1] = 1\n",
    "    return X, y\n",
    "\n",
    "def train(GAN, G, D, epochs=500, n_samples=10000, noise_dim=10, batch_size=32, verbose=False, v_freq=50):\n",
    "    d_loss = []\n",
    "    g_loss = []\n",
    "    e_range = range(epochs)\n",
    "    if verbose:\n",
    "        e_range = tqdm(e_range)\n",
    "    for epoch in e_range:\n",
    "        X, y = sample_data_and_gen(G, n_samples=n_samples, noise_dim=noise_dim)\n",
    "        set_trainability(D, True)\n",
    "        d_loss.append(D.train_on_batch(X, y))\n",
    "        \n",
    "        X, y = sample_noise(G, n_samples=n_samples, noise_dim=noise_dim)\n",
    "        set_trainability(D, False)\n",
    "        g_loss.append(GAN.train_on_batch(X, y))\n",
    "        if verbose and (epoch + 1) % v_freq == 0:\n",
    "            print(\"Epoch #{}: Generative Loss: {}, Discriminative Loss: {}\".format(epoch + 1, g_loss[-1], d_loss[-1]))\n",
    "    return d_loss, g_loss\n",
    "\n",
    "d_loss, g_loss = train(GAN, G, D, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = pd.DataFrame(\n",
    "    {\n",
    "        'Generative Loss': g_loss,\n",
    "        'Discriminative Loss': d_loss,\n",
    "    }\n",
    ").plot(title='Training loss', logy=True)\n",
    "ax.set_xlabel(\"Epochs\")\n",
    "ax.set_ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_VIEWED_SAMPLES = 2\n",
    "data_and_gen, _ = sample_data_and_gen(G, n_samples=N_VIEWED_SAMPLES)\n",
    "pd.DataFrame(np.transpose(data_and_gen[N_VIEWED_SAMPLES:])).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_VIEWED_SAMPLES = 2\n",
    "data_and_gen, _ = sample_data_and_gen(G, n_samples=N_VIEWED_SAMPLES)\n",
    "pd.DataFrame(np.transpose(data_and_gen[N_VIEWED_SAMPLES:])).rolling(5).mean()[5:].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
