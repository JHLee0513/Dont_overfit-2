{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving our log reg model for better improvement\n",
    "\n",
    "So we saw even blending a log reg model initially at 0.849 with lasso model of 0.868 improves at 0.869! We will see if improving our log reg score will help towards that. \n",
    "\n",
    "\n",
    "Hence, let's try implementing bayesian methods to logistic regression. To do so we will try two different implementations - pymc3 and pystan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pystan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from boruta import BorutaPy\n",
    "from sklearn.linear_model import Lasso, LogisticRegression\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, roc_auc_score, r2_score, make_scorer\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "import theano.tensor as t\n",
    "from scipy.stats import mode\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "train = pd.read_csv(\"/Users/JoonH/dont-overfit-ii/train.csv\")\n",
    "train_y = train['target'].astype(int)\n",
    "train_X = train.drop(['id','target'], axis=1).values\n",
    "\n",
    "test_df = pd.read_csv(\"/Users/JoonH/dont-overfit-ii/test.csv\")\n",
    "test = test_df.drop(['id'], axis=1).values\n",
    "\n",
    "# scale using RobustScaler\n",
    "# fitting scaler on full data outperforms fitting on test_X only (+0.006 kaggle score)\n",
    "data = RobustScaler().fit_transform(np.concatenate((train_X, test), axis=0))\n",
    "train_X = data[:250]\n",
    "test = data[250:]\n",
    "# add a bit of noise to train_X to reduce overfitting\n",
    "train_X += np.random.normal(0, 0.01, train_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pystan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_ff9f6282f9e65bf0bbfafc8f9821850f NOW.\n",
      "WARNING:pystan:Maximum (flat) parameter count (1000) exceeded: skipping diagnostic tests for n_eff and Rhat.\n",
      "To run all diagnostics call pystan.check_hmc_diagnostics(fit)\n"
     ]
    }
   ],
   "source": [
    "#https://www.kaggle.com/gkoundry/bayesian-logistic-regression-with-pystan                                                                                                     \n",
    "code = \"\"\"                                                                                           \n",
    "data {                                                                                               \n",
    "  int N; //the number of training observations                                                       \n",
    "  int N2; //the number of test observations                                                          \n",
    "  int K; //the number of features                                                                    \n",
    "  int y[N]; //the response                                                                           \n",
    "  matrix[N,K] X; //the model matrix                                                                  \n",
    "  matrix[N2,K] new_X; //the matrix for the predicted values                                          \n",
    "}                                                                                                    \n",
    "parameters {                                                                                         \n",
    "  real alpha;                                                                                        \n",
    "  vector[K] beta; //the regression parameters                                                        \n",
    "}                                                                                                    \n",
    "transformed parameters {                                                                             \n",
    "  vector[N] linpred;                                                                                 \n",
    "  linpred = alpha+X*beta;                                                                            \n",
    "}                                                                                                    \n",
    "model {                                                                                              \n",
    "  alpha ~ cauchy(0,13); //prior for the intercept following Gelman 2008\n",
    "  //originally cauchy(0, 10) but 13 gives better score\n",
    "                                                                                                     \n",
    "  for(i in 1:K)                                                                                      \n",
    "    beta[i] ~ student_t(1, 0, 0.03);                                                                 \n",
    "                                                                                                     \n",
    "  y ~ bernoulli_logit(linpred);                                                                      \n",
    "}                                                                                                    \n",
    "generated quantities {                                                                               \n",
    "  vector[N2] y_pred;                                                                                 \n",
    "  y_pred = alpha+new_X*beta; //the y values predicted by the model                                   \n",
    "}                                                                                                    \n",
    "\"\"\"    \n",
    "\n",
    "data = {                                                                                             \n",
    "    'N': 250,                                                                                        \n",
    "    'N2': 19750,                                                                                     \n",
    "    'K': 300,                                                                                        \n",
    "    'y': train_y,                                                                                     \n",
    "    'X': train_X,                                                                                      \n",
    "    'new_X': test,                                                                                   \n",
    "} \n",
    "\n",
    "n_itr = 3000\n",
    "n_warmup = 1000\n",
    "\n",
    "sm = pystan.StanModel(model_code = code)\n",
    "fit = sm.sampling(data = data, iter = n_itr, warmup = n_warmup, seed = 2019)\n",
    "ex = fit.extract(permuted = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from scipy.stats import bernoulli\n",
    "\n",
    "def logit_to_prob(logit):\n",
    "    odds = np.exp(logit)\n",
    "    prob = odds / (1 + odds)\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#target = np.mean(ex['y_pred'], axis = 0)\n",
    "target = np.mean(logit_to_prob(ex['y_pred']), axis = 0)\n",
    "ids = test_df['id']\n",
    "df = pd.DataFrame({'id': ids, 'target' : target})\n",
    "df[['id', 'target']].to_csv(\"/Users/JoonH/DO2_pystan_log.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us LB score of 0.860"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*sources:*\n",
    "\n",
    "1. https://www.kaggle.com/gkoundry/bayesian-logistic-regression-with-pystan/log\n",
    "2. https://barnesanalytics.com/bayesian-logistic-regression-in-python-using-pymc3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
