{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving our log reg model for better improvement\n",
    "\n",
    "So we saw even blending a log reg model initially at 0.849 with lasso model of 0.868 improves at 0.869! We will see if improving our log reg score will help towards that. \n",
    "\n",
    "\n",
    "Hence, let's try implementing bayesian methods to logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install numpy --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import Lasso, LogisticRegression\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, roc_auc_score, r2_score, make_scorer\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "#import theano.tensor as t\n",
    "#from scipy.stats import mode\n",
    "#import pymc3 as pm\n",
    "\n",
    "#def warn(*args, **kwargs):\n",
    "#    pass\n",
    "#import warnings\n",
    "#warnings.warn = warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "train = pd.read_csv(\"/Users/JoonH/dont-overfit-ii/train.csv\")\n",
    "train_y = train['target']\n",
    "train_X = train.drop(['id','target'], axis=1).values\n",
    "\n",
    "test = pd.read_csv(\"/Users/JoonH/dont-overfit-ii/test.csv\")\n",
    "test = test.drop(['id'], axis=1).values\n",
    "\n",
    "# scale using RobustScaler\n",
    "# fitting scaler on full data outperforms fitting on test_X only (+0.006 kaggle score)\n",
    "data = RobustScaler().fit_transform(np.concatenate((train_X, test), axis=0))\n",
    "#scaler = RobustScaler().fit(train_X)\n",
    "#train_X = scaler.transform(train_X)\n",
    "train_X = data[:250]\n",
    "test = data[250:]\n",
    "#test = scaler.transform(test)\n",
    "# add a bit of noise to train_X to reduce overfitting\n",
    "train_X += np.random.normal(0, 0.01, train_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heuristics\n",
    "rfe_min_features = 20\n",
    "rfe_step = 5\n",
    "rfe_cv = 20\n",
    "sss_n_splits = 20\n",
    "sss_test_size = 0.35\n",
    "grid_search_cv = 20\n",
    "noise_std = 0.01\n",
    "r2_threshold = 0.185\n",
    "roc_threshold = 0.7\n",
    "random_seed = 213\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-11-a168df76e4bb>, line 29)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-11-a168df76e4bb>\"\u001b[1;36m, line \u001b[1;32m29\u001b[0m\n\u001b[1;33m    'leraning_rate' : [0.01],\u001b[0m\n\u001b[1;37m                  ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# define roc_auc_metric robust to only one class in y_pred\n",
    "def scoring_roc_auc(y, y_pred):\n",
    "    try:\n",
    "        return roc_auc_score(y, y_pred)\n",
    "    except:\n",
    "        return 0.5\n",
    "\n",
    "robust_roc_auc = make_scorer(scoring_roc_auc)\n",
    "\n",
    "# define model and its parameters\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "'''\n",
    "model = LogisticRegression(penalty='l1', tol=0.00001, C=0.1, fit_intercept=False,\n",
    "                           intercept_scaling=1, class_weight='balanced', random_state=213, \n",
    "                           max_iter=50000, solver = 'liblinear')\n",
    "param_grid = {\n",
    "            'C' : [0.09, 0.0925, 0.095, 0.0975, 0.1, 0.1025, 0.105, 0.11],\n",
    "            'tol'   : [1e-04],\n",
    "            #'intercept_scaling' : [0.5,0.75,1,1.25,1.5]\n",
    "            'penalty' : ['l1', 'l2'],\n",
    "            'class_weight' : ['balanced',None],\n",
    "            'solver' : ['liblinear']\n",
    "        }'''\n",
    "\n",
    "model = XGBClassifier(max_depth = 4, learning_rate = 0.1, n_estimators = 100, seed = random_seed)\n",
    "param_grid = {\n",
    "            'max_depth' : [4]\n",
    "            'leraning_rate' : [0.01],\n",
    "            'n_estimators'   : [500],\n",
    "        }\n",
    "# define recursive elimination feature selector\n",
    "feature_selector = RFECV(model, min_features_to_select=rfe_min_features, scoring=robust_roc_auc, step=rfe_step, verbose=0, cv=rfe_cv, n_jobs=-1)\n",
    "\n",
    "print(\"counter | val_mse  |  val_mae  |  val_roc  |  val_cos  |  val_dist  |  val_r2    | feature_count \")\n",
    "print(\"-------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "predictions = pd.DataFrame()\n",
    "counter = 0\n",
    "# split training data to build one model on each traing-data-subset\n",
    "# TODO: turn this into a method to ensemble different models\n",
    "for train_index, val_index in StratifiedShuffleSplit(n_splits=sss_n_splits, test_size=sss_test_size, random_state=random_seed).split(train_X, train_y):\n",
    "    X, val_X = train_X[train_index], train_X[val_index]\n",
    "    y, val_y = train_y[train_index], train_y[val_index]\n",
    "\n",
    "    # get the best features for this data set\n",
    "    feature_selector.fit(X, y)\n",
    "    # remove irrelevant features from X, val_X and test\n",
    "    X_important_features        = feature_selector.transform(X)\n",
    "    val_X_important_features    = feature_selector.transform(val_X)\n",
    "    test_important_features     = feature_selector.transform(test)\n",
    "\n",
    "    # run grid search to find the best Lasso parameters for this subset of training data and subset of features \n",
    "    grid_search = GridSearchCV(feature_selector.estimator_, param_grid=param_grid, verbose=0, n_jobs=-1, scoring=robust_roc_auc, cv=grid_search_cv, iid = False)\n",
    "    grid_search.fit(X_important_features, y)\n",
    "    print(grid_search.best_params_)\n",
    "\n",
    "    # score our fitted model on validation data\n",
    "    val_y_pred = grid_search.best_estimator_.predict(val_X_important_features)\n",
    "    val_mse = mean_squared_error(val_y, val_y_pred)\n",
    "    val_mae = mean_absolute_error(val_y, val_y_pred)\n",
    "    val_roc = roc_auc_score(val_y, val_y_pred)\n",
    "    val_cos = cosine_similarity(val_y.values.reshape(1, -1), val_y_pred.reshape(1, -1))[0][0]\n",
    "    val_dst = euclidean_distances(val_y.values.reshape(1, -1), val_y_pred.reshape(1, -1))[0][0]\n",
    "    val_r2  = r2_score(val_y, val_y_pred)\n",
    "\n",
    "    # if model did well on validation, save its prediction on test data, using only important features\n",
    "    # r2_threshold (0.185) is a heuristic threshold for r2 error\n",
    "    # you can use any other metric/metric combination that works for you\n",
    "    #if val_r2 > r2_threshold:\n",
    "    if val_roc > roc_threshold:\n",
    "        message = '<-- OK'\n",
    "        prediction = grid_search.best_estimator_.predict(test_important_features)\n",
    "        predictions = pd.concat([predictions, pd.DataFrame(prediction)], axis=1)\n",
    "    else:\n",
    "        message = '<-- skipping'\n",
    "\n",
    "\n",
    "    print(\"{0:2}      | {1:.4f}   |  {2:.4f}   |  {3:.4f}   |  {4:.4f}   |  {5:.4f}    |  {6:.4f}    |  {7:3}         {8}  \".format(counter, val_mse, val_mae, val_roc, val_cos, val_dst, val_r2, feature_selector.n_features_, message))\n",
    "    \n",
    "    counter += 1\n",
    "\n",
    "print(\"-------------------------------------------------------------------------------------------------\")\n",
    "print(\"{}/{} models passed validation threshold and will be ensembled.\".format(len(predictions.columns), sss_n_splits))\n",
    "\n",
    "mean_pred = pd.DataFrame(predictions.mean(axis=1))\n",
    "mean_pred.index += 250\n",
    "mean_pred.columns = ['target']\n",
    "mean_pred.to_csv('dont_overfit2_xgb_robust.csv', index_label='id', index=True)        \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://barnesanalytics.com/bayesian-logistic-regression-in-python-using-pymc3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
