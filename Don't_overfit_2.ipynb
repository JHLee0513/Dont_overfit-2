{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Don't overfit 2!\n",
    "\n",
    "This is my approach to Kaggle's competition for don't overfit 2. This is quite an interesting problem, for we are given only 250 training samples while we are given 20K+ testing examples. Essentially, our goal is to build a model robust enough that it won't overfit to our training samples. \n",
    "\n",
    "While the 'traditional' ML models are performing very well with best being logistic regression at about 0.8+ at current observation, I try to push Hinton's CapsNet to see if we can perform as well or even better. For best performance I am expecting to an increase with a right blending of Capsnet, logistic reggresion, and perhaps a few more models. Here we go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is happening so far\n",
    "* Best model is logistic reg model\n",
    "* permutation importance gives us good idea of important features, but fitting to the most important features lead to fast overfitting\n",
    "* bootstrapping a small amount of test data to train dataset help generalize, but amounts beyond 20~ gives too much unstability\n",
    "\n",
    "Current Plan:\n",
    "* CapsNet\n",
    "* blending of models other than log reg\n",
    "* stacknet?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "445"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.layers import Layer\n",
    "from keras.layers import *\n",
    "from keras.metrics import *\n",
    "from keras.models import Model\n",
    "from keras.callbacks import *\n",
    "from keras.optimizers import *\n",
    "from keras.applications import *\n",
    "from keras import activations\n",
    "from keras import utils\n",
    "from keras.regularizers import l2\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold, cross_val_score, GridSearchCV, RepeatedStratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import gc\n",
    "gc.enable()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"/Users/JoonH/dont-overfit-ii/train.csv\")\n",
    "test_df = pd.read_csv(\"/Users/JoonH/dont-overfit-ii/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>2.165</td>\n",
       "      <td>0.681</td>\n",
       "      <td>-0.614</td>\n",
       "      <td>1.309</td>\n",
       "      <td>-0.455</td>\n",
       "      <td>-0.236</td>\n",
       "      <td>0.276</td>\n",
       "      <td>...</td>\n",
       "      <td>0.867</td>\n",
       "      <td>1.347</td>\n",
       "      <td>0.504</td>\n",
       "      <td>-0.649</td>\n",
       "      <td>0.672</td>\n",
       "      <td>-2.097</td>\n",
       "      <td>1.051</td>\n",
       "      <td>-0.414</td>\n",
       "      <td>1.038</td>\n",
       "      <td>-1.065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.081</td>\n",
       "      <td>-0.973</td>\n",
       "      <td>-0.383</td>\n",
       "      <td>0.326</td>\n",
       "      <td>-0.428</td>\n",
       "      <td>0.317</td>\n",
       "      <td>1.172</td>\n",
       "      <td>0.352</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.165</td>\n",
       "      <td>-1.695</td>\n",
       "      <td>-1.257</td>\n",
       "      <td>1.359</td>\n",
       "      <td>-0.808</td>\n",
       "      <td>-1.624</td>\n",
       "      <td>-0.458</td>\n",
       "      <td>-1.099</td>\n",
       "      <td>-0.936</td>\n",
       "      <td>0.973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.523</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>-0.348</td>\n",
       "      <td>0.148</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>0.404</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.172</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.263</td>\n",
       "      <td>-1.222</td>\n",
       "      <td>0.726</td>\n",
       "      <td>1.444</td>\n",
       "      <td>-1.165</td>\n",
       "      <td>-1.544</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.800</td>\n",
       "      <td>-1.211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.067</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.392</td>\n",
       "      <td>-1.637</td>\n",
       "      <td>-0.446</td>\n",
       "      <td>-0.725</td>\n",
       "      <td>-1.035</td>\n",
       "      <td>0.834</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>0.640</td>\n",
       "      <td>-0.595</td>\n",
       "      <td>-0.966</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.467</td>\n",
       "      <td>-0.562</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.533</td>\n",
       "      <td>0.238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.347</td>\n",
       "      <td>-0.831</td>\n",
       "      <td>0.511</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>1.225</td>\n",
       "      <td>1.594</td>\n",
       "      <td>0.585</td>\n",
       "      <td>1.509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.898</td>\n",
       "      <td>0.134</td>\n",
       "      <td>2.415</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-1.006</td>\n",
       "      <td>1.378</td>\n",
       "      <td>1.246</td>\n",
       "      <td>1.478</td>\n",
       "      <td>0.428</td>\n",
       "      <td>0.253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 302 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target      0      1      2      3      4      5      6      7  ...    \\\n",
       "0   0     1.0 -0.098  2.165  0.681 -0.614  1.309 -0.455 -0.236  0.276  ...     \n",
       "1   1     0.0  1.081 -0.973 -0.383  0.326 -0.428  0.317  1.172  0.352  ...     \n",
       "2   2     1.0 -0.523 -0.089 -0.348  0.148 -0.022  0.404 -0.023 -0.172  ...     \n",
       "3   3     1.0  0.067 -0.021  0.392 -1.637 -0.446 -0.725 -1.035  0.834  ...     \n",
       "4   4     1.0  2.347 -0.831  0.511 -0.021  1.225  1.594  0.585  1.509  ...     \n",
       "\n",
       "     290    291    292    293    294    295    296    297    298    299  \n",
       "0  0.867  1.347  0.504 -0.649  0.672 -2.097  1.051 -0.414  1.038 -1.065  \n",
       "1 -0.165 -1.695 -1.257  1.359 -0.808 -1.624 -0.458 -1.099 -0.936  0.973  \n",
       "2  0.013  0.263 -1.222  0.726  1.444 -1.165 -1.544  0.004  0.800 -1.211  \n",
       "3 -0.404  0.640 -0.595 -0.966  0.900  0.467 -0.562 -0.254 -0.533  0.238  \n",
       "4  0.898  0.134  2.415 -0.996 -1.006  1.378  1.246  1.478  0.428  0.253  \n",
       "\n",
       "[5 rows x 302 columns]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 250 entries, 0 to 249\n",
      "Columns: 302 entries, id to 299\n",
      "dtypes: float64(301), int64(1)\n",
      "memory usage: 589.9 KB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>250</td>\n",
       "      <td>0.500</td>\n",
       "      <td>-1.033</td>\n",
       "      <td>-1.595</td>\n",
       "      <td>0.309</td>\n",
       "      <td>-0.714</td>\n",
       "      <td>0.502</td>\n",
       "      <td>0.535</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>-0.687</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-2.628</td>\n",
       "      <td>-0.845</td>\n",
       "      <td>2.078</td>\n",
       "      <td>-0.277</td>\n",
       "      <td>2.132</td>\n",
       "      <td>0.609</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>251</td>\n",
       "      <td>0.776</td>\n",
       "      <td>0.914</td>\n",
       "      <td>-0.494</td>\n",
       "      <td>1.347</td>\n",
       "      <td>-0.867</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.578</td>\n",
       "      <td>-0.313</td>\n",
       "      <td>0.203</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.683</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.606</td>\n",
       "      <td>-0.353</td>\n",
       "      <td>-1.133</td>\n",
       "      <td>-3.138</td>\n",
       "      <td>0.281</td>\n",
       "      <td>-0.625</td>\n",
       "      <td>-0.761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>252</td>\n",
       "      <td>1.750</td>\n",
       "      <td>0.509</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>0.835</td>\n",
       "      <td>-0.476</td>\n",
       "      <td>1.428</td>\n",
       "      <td>-0.701</td>\n",
       "      <td>-2.009</td>\n",
       "      <td>-1.378</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>0.351</td>\n",
       "      <td>-0.607</td>\n",
       "      <td>-0.737</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>0.701</td>\n",
       "      <td>0.976</td>\n",
       "      <td>0.135</td>\n",
       "      <td>-1.327</td>\n",
       "      <td>2.463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>253</td>\n",
       "      <td>-0.556</td>\n",
       "      <td>-1.855</td>\n",
       "      <td>-0.682</td>\n",
       "      <td>0.578</td>\n",
       "      <td>1.592</td>\n",
       "      <td>0.512</td>\n",
       "      <td>-1.419</td>\n",
       "      <td>0.722</td>\n",
       "      <td>0.511</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.336</td>\n",
       "      <td>-0.787</td>\n",
       "      <td>0.255</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>-0.836</td>\n",
       "      <td>0.916</td>\n",
       "      <td>2.411</td>\n",
       "      <td>1.053</td>\n",
       "      <td>-1.601</td>\n",
       "      <td>-1.529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>254</td>\n",
       "      <td>0.754</td>\n",
       "      <td>-0.245</td>\n",
       "      <td>1.173</td>\n",
       "      <td>-1.623</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.370</td>\n",
       "      <td>0.781</td>\n",
       "      <td>-1.763</td>\n",
       "      <td>-1.432</td>\n",
       "      <td>...</td>\n",
       "      <td>2.184</td>\n",
       "      <td>-1.090</td>\n",
       "      <td>0.216</td>\n",
       "      <td>1.186</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>0.322</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>-1.153</td>\n",
       "      <td>0.825</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 301 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    id      0      1      2      3      4      5      6      7      8  ...    \\\n",
       "0  250  0.500 -1.033 -1.595  0.309 -0.714  0.502  0.535 -0.129 -0.687  ...     \n",
       "1  251  0.776  0.914 -0.494  1.347 -0.867  0.480  0.578 -0.313  0.203  ...     \n",
       "2  252  1.750  0.509 -0.057  0.835 -0.476  1.428 -0.701 -2.009 -1.378  ...     \n",
       "3  253 -0.556 -1.855 -0.682  0.578  1.592  0.512 -1.419  0.722  0.511  ...     \n",
       "4  254  0.754 -0.245  1.173 -1.623  0.009  0.370  0.781 -1.763 -1.432  ...     \n",
       "\n",
       "     290    291    292    293    294    295    296    297    298    299  \n",
       "0 -0.088 -2.628 -0.845  2.078 -0.277  2.132  0.609 -0.104  0.312  0.979  \n",
       "1 -0.683 -0.066  0.025  0.606 -0.353 -1.133 -3.138  0.281 -0.625 -0.761  \n",
       "2 -0.094  0.351 -0.607 -0.737 -0.031  0.701  0.976  0.135 -1.327  2.463  \n",
       "3 -0.336 -0.787  0.255 -0.031 -0.836  0.916  2.411  1.053 -1.601 -1.529  \n",
       "4  2.184 -1.090  0.216  1.186 -0.143  0.322 -0.068 -0.156 -1.153  0.825  \n",
       "\n",
       "[5 rows x 301 columns]"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our data we see that we have 300 continous variables, let's see if we can get any understanding of the data through some EDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sum EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our model(s)\n",
    "\n",
    "We will use a capsnet inspired NN and a logistic regression model. For our NN we will also try to implement the idea of self-normalizing networks, or SNN, and blend the output probabilities with the logreg model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_df['target']\n",
    "x_train = train_df.drop(['target', 'id'], axis = 1)\n",
    "x_test = test_df.drop(['id'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_nn = x_train.astype('float32')\n",
    "x_train_nn = np.expand_dims(x_train, axis = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250, 300, 1)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#250 samples with each sample containing 300 variables, we expand dims such that it will fit our NN model\n",
    "x_train_nn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250, 300)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training data for the logreg model\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use basic capsule implementation provided by Keras\n",
    "\n",
    "# the squashing function.\n",
    "# we use 0.5 in stead of 1 in hinton's paper.\n",
    "# if 1, the norm of vector will be zoomed out.\n",
    "# if 0.5, the norm will be zoomed in while original norm is less than 0.5\n",
    "# and be zoomed out while original norm is greater than 0.5.\n",
    "def squash(x, axis=-1):\n",
    "    s_squared_norm = K.sum(K.square(x), axis, keepdims=True) + K.epsilon()\n",
    "    scale = K.sqrt(s_squared_norm) / (0.5 + s_squared_norm)\n",
    "    return scale * x\n",
    "\n",
    "\n",
    "# define our own softmax function instead of K.softmax\n",
    "# because K.softmax can not specify axis.\n",
    "def softmax(x, axis=-1):\n",
    "    ex = K.exp(x - K.max(x, axis=axis, keepdims=True))\n",
    "    return ex / K.sum(ex, axis=axis, keepdims=True)\n",
    "\n",
    "\n",
    "# define the margin loss like hinge loss\n",
    "def margin_loss(y_true, y_pred):\n",
    "    lamb, margin = 0.5, 0.1\n",
    "    return K.sum(y_true * K.square(K.relu(1 - margin - y_pred)) + lamb * (\n",
    "        1 - y_true) * K.square(K.relu(y_pred - margin)), axis=-1)\n",
    "\n",
    "\n",
    "class Capsule(Layer):\n",
    "    \"\"\"A Capsule Implement with Pure Keras\n",
    "    There are two vesions of Capsule.\n",
    "    One is like dense layer (for the fixed-shape input),\n",
    "    and the other is like timedistributed dense (for various length input).\n",
    "\n",
    "    The input shape of Capsule must be (batch_size,\n",
    "                                        input_num_capsule,\n",
    "                                        input_dim_capsule\n",
    "                                       )\n",
    "    and the output shape is (batch_size,\n",
    "                             num_capsule,\n",
    "                             dim_capsule\n",
    "                            )\n",
    "\n",
    "    Capsule Implement is from https://github.com/bojone/Capsule/\n",
    "    Capsule Paper: https://arxiv.org/abs/1710.09829\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_capsule,\n",
    "                 dim_capsule,\n",
    "                 routings=3,\n",
    "                 share_weights=True,\n",
    "                 activation='squash',\n",
    "                 **kwargs):\n",
    "        super(Capsule, self).__init__(**kwargs)\n",
    "        self.num_capsule = num_capsule\n",
    "        self.dim_capsule = dim_capsule\n",
    "        self.routings = routings\n",
    "        self.share_weights = share_weights\n",
    "        if activation == 'squash':\n",
    "            self.activation = squash\n",
    "        else:\n",
    "            self.activation = activations.get(activation)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        input_dim_capsule = input_shape[-1]\n",
    "        if self.share_weights:\n",
    "            self.kernel = self.add_weight(\n",
    "                name='capsule_kernel',\n",
    "                shape=(1, input_dim_capsule,\n",
    "                       self.num_capsule * self.dim_capsule),\n",
    "                initializer='glorot_uniform',\n",
    "                trainable=True)\n",
    "        else:\n",
    "            input_num_capsule = input_shape[-2]\n",
    "            self.kernel = self.add_weight(\n",
    "                name='capsule_kernel',\n",
    "                shape=(input_num_capsule, input_dim_capsule,\n",
    "                       self.num_capsule * self.dim_capsule),\n",
    "                initializer='glorot_uniform',\n",
    "                trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"Following the routing algorithm from Hinton's paper,\n",
    "        but replace b = b + <u,v> with b = <u,v>.\n",
    "\n",
    "        This change can improve the feature representation of Capsule.\n",
    "\n",
    "        However, you can replace\n",
    "            b = K.batch_dot(outputs, hat_inputs, [2, 3])\n",
    "        with\n",
    "            b += K.batch_dot(outputs, hat_inputs, [2, 3])\n",
    "        to realize a standard routing.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.share_weights:\n",
    "            hat_inputs = K.conv1d(inputs, self.kernel)\n",
    "        else:\n",
    "            hat_inputs = K.local_conv1d(inputs, self.kernel, [1], [1])\n",
    "\n",
    "        batch_size = K.shape(inputs)[0]\n",
    "        input_num_capsule = K.shape(inputs)[1]\n",
    "        hat_inputs = K.reshape(hat_inputs,\n",
    "                               (batch_size, input_num_capsule,\n",
    "                                self.num_capsule, self.dim_capsule))\n",
    "        hat_inputs = K.permute_dimensions(hat_inputs, (0, 2, 1, 3))\n",
    "\n",
    "        b = K.zeros_like(hat_inputs[:, :, :, 0])\n",
    "        for i in range(self.routings):\n",
    "            c = softmax(b, 1)\n",
    "            o = self.activation(K.batch_dot(c, hat_inputs, [2, 2]))\n",
    "            if i < self.routings - 1:\n",
    "                b = K.batch_dot(o, hat_inputs, [2, 3])\n",
    "                if K.backend() == 'theano':\n",
    "                    o = K.sum(o, axis=1)\n",
    "\n",
    "        return o\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, self.num_capsule, self.dim_capsule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mask(layers.Layer):\n",
    "    \"\"\"\n",
    "    Mask a Tensor with shape=[None, num_capsule, dim_vector] either by the capsule with max length or by an additional \n",
    "    input mask. Except the max-length capsule (or specified capsule), all vectors are masked to zeros. Then flatten the\n",
    "    masked Tensor.\n",
    "    For example:\n",
    "        ```\n",
    "        x = keras.layers.Input(shape=[8, 3, 2])  # batch_size=8, each sample contains 3 capsules with dim_vector=2\n",
    "        y = keras.layers.Input(shape=[8, 3])  # True labels. 8 samples, 3 classes, one-hot coding.\n",
    "        out = Mask()(x)  # out.shape=[8, 6]\n",
    "        # or\n",
    "        out2 = Mask()([x, y])  # out2.shape=[8,6]. Masked with true labels y. Of course y can also be manipulated.\n",
    "        ```\n",
    "    \"\"\"\n",
    "    def call(self, inputs, **kwargs):\n",
    "        if type(inputs) is list:  # true label is provided with shape = [None, n_classes], i.e. one-hot code.\n",
    "            assert len(inputs) == 2\n",
    "            inputs, mask = inputs\n",
    "        else:  # if no true label, mask by the max length of capsules. Mainly used for prediction\n",
    "            # compute lengths of capsules\n",
    "            x = K.sqrt(K.sum(K.square(inputs), -1))\n",
    "            # generate the mask which is a one-hot code.\n",
    "            # mask.shape=[None, n_classes]=[None, num_capsule]\n",
    "            mask = K.one_hot(indices=K.argmax(x, 1), num_classes=x.get_shape().as_list()[1])\n",
    "\n",
    "        # inputs.shape=[None, num_capsule, dim_capsule]\n",
    "        # mask.shape=[None, num_capsule]\n",
    "        # masked.shape=[None, num_capsule * dim_capsule]\n",
    "        masked = K.batch_flatten(inputs * K.expand_dims(mask, -1))\n",
    "        return masked\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if type(input_shape[0]) is tuple:  # true label provided\n",
    "            return tuple([None, input_shape[0][1] * input_shape[0][2]])\n",
    "        else:  # no true label provided\n",
    "            return tuple([None, input_shape[1] * input_shape[2]])\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(Mask, self).get_config()\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = l2(0.5)\n",
    "\n",
    "inputs = Input(shape = (300,1))\n",
    "x = Conv1D(256, (10), activation='elu', kernel_initializer = 'glorot_normal', kernel_regularizer=reg)(inputs)\n",
    "x = Conv1D(128, (10), activation='elu', kernel_initializer = 'glorot_normal', kernel_regularizer=reg)(x)\n",
    "#x = AveragePooling2D((2, 2))(x)\n",
    "#x = Conv1D(96, (5), activation='elu', kernel_initializer = 'glorot_normal', kernel_regularizer=reg)(x)\n",
    "x = Conv1D(64, (5), activation='elu', kernel_initializer = 'glorot_normal', kernel_regularizer=reg)(x)\n",
    "\n",
    "\n",
    "capsule = Capsule(1, 32, 3, True)(x)\n",
    "cap = Lambda(lambda z: K.sqrt(K.sum(K.square(z), 2)))(capsule)\n",
    "\n",
    "#decoder\n",
    "#y = Input(shape=(2,))\n",
    "#masked_by_y = Mask()([capsule,y])\n",
    "#masked = Mask()(capsule)\n",
    "\n",
    "#decoder = models.Sequential(name = 'decoder')\n",
    "#decoder.add(Dense(128, activation='relu', input_dim = 16*2))\n",
    "#decoder.add(Dense(256, activation='relu'))\n",
    "#decoder.add(Dense(300, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_45 (InputLayer)        (None, 300, 1)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_114 (Conv1D)          (None, 291, 256)          2816      \n",
      "_________________________________________________________________\n",
      "conv1d_115 (Conv1D)          (None, 282, 128)          327808    \n",
      "_________________________________________________________________\n",
      "conv1d_116 (Conv1D)          (None, 278, 64)           41024     \n",
      "_________________________________________________________________\n",
      "capsule_36 (Capsule)         (None, 1, 32)             2048      \n",
      "_________________________________________________________________\n",
      "lambda_36 (Lambda)           (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 373,696\n",
      "Trainable params: 373,696\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs, cap)\n",
    "#model = models.Model([inputs,y],[cap, decoder(masked_by_y)]) #model for training\n",
    "\n",
    "#eval_model = models.Model(inputs, [cap, decoder(masked)]) #eval_model for prediction\n",
    "\n",
    "#cap = Flatten()(cap)\n",
    "#drop = DropConnect(Dense(32, activation=\"elu\"), prob=0.5)(cap)\n",
    "#output = Dense(1, activation = 'sigmoid') (drop)\n",
    "\n",
    "\n",
    "# we use a margin loss\n",
    "model.compile(loss=[margin_loss], optimizer='adamax', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_42 (InputLayer)           (None, 300, 1)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_108 (Conv1D)             (None, 291, 256)     2816        input_42[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_109 (Conv1D)             (None, 282, 128)     327808      conv1d_108[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_110 (Conv1D)             (None, 278, 64)      41024       conv1d_109[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "capsule_34 (Capsule)            (None, 2, 16)        2048        conv1d_110[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "input_43 (InputLayer)           (None, 2)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "mask_8 (Mask)                   (None, 32)           0           capsule_34[0][0]                 \n",
      "                                                                 input_43[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_34 (Lambda)              (None, 2)            0           capsule_34[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Sequential)            (None, 300)          114348      mask_8[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 488,044\n",
      "Trainable params: 488,044\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "eval_model.compile(loss=[margin_loss, 'mse'], optimizer='adamax', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CapsNet-Keras https://github.com/XifengGuo/CapsNet-Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stratified K fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "n_fold = 20\n",
    "folds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=42)\n",
    "repeated_folds = RepeatedStratifiedKFold(n_splits=n_fold, n_repeats=20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X, X_test, y, params, folds=folds, model_type='lgb', plot_feature_importance=False, averaging='usual', model=None):\n",
    "    oof = np.zeros(len(X))\n",
    "    prediction = np.zeros(len(X_test))\n",
    "    scores = []\n",
    "    feature_importance = pd.DataFrame()\n",
    "    for fold_n, (train_index, valid_index) in enumerate(folds.split(X, y)):\n",
    "        # print('Fold', fold_n, 'started at', time.ctime())\n",
    "        X_train, X_valid = X[train_index], X[valid_index]\n",
    "        y_train, y_valid = y[train_index], y[valid_index]\n",
    "        \n",
    "        if model_type == 'lgb':\n",
    "            train_data = lgb.Dataset(X_train, label=y_train)\n",
    "            valid_data = lgb.Dataset(X_valid, label=y_valid)\n",
    "            \n",
    "            model = lgb.train(params,\n",
    "                    train_data,\n",
    "                    num_boost_round=2000,\n",
    "                    valid_sets = [train_data, valid_data],\n",
    "                    verbose_eval=500,\n",
    "                    early_stopping_rounds = 200)\n",
    "            \n",
    "            y_pred_valid = model.predict(X_valid)\n",
    "            y_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n",
    "            \n",
    "        if model_type == 'xgb':\n",
    "            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X_tr.columns)\n",
    "            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X_tr.columns)\n",
    "\n",
    "            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n",
    "            model = xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200, verbose_eval=500, params=params)\n",
    "            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X_tr.columns), ntree_limit=model.best_ntree_limit)\n",
    "            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X_tr.columns), ntree_limit=model.best_ntree_limit)\n",
    "        \n",
    "        if model_type == 'cat':\n",
    "            model = CatBoostClassifier(iterations=20000,  eval_metric='AUC', **params)\n",
    "            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n",
    "\n",
    "            y_pred_valid = model.predict(X_valid)\n",
    "            y_pred = model.predict(X_test)\n",
    "            \n",
    "            \n",
    "        if model_type == 'sklearn':\n",
    "            model = model\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred_valid = model.predict(X_valid).reshape(-1,)\n",
    "            score = roc_auc_score(y_valid, y_pred_valid)\n",
    "            # print(f'Fold {fold_n}. AUC: {score:.4f}.')\n",
    "            # print('')\n",
    "            \n",
    "            y_pred = model.predict_proba(X_test)[:, 1]\n",
    "            \n",
    "        if model_type == 'glm':\n",
    "            model = sm.GLM(y_train, X_train, family=sm.families.Binomial())\n",
    "            model_results = model.fit()\n",
    "            model_results.predict(X_test)\n",
    "            y_pred_valid = model_results.predict(X_valid).reshape(-1,)\n",
    "            score = roc_auc_score(y_valid, y_pred_valid)\n",
    "            \n",
    "            y_pred = model_results.predict(X_test)\n",
    "        \n",
    "        oof[valid_index] = y_pred_valid.reshape(-1,)\n",
    "        scores.append(roc_auc_score(y_valid, y_pred_valid))\n",
    "\n",
    "        if averaging == 'usual':\n",
    "            prediction += y_pred\n",
    "        elif averaging == 'rank':\n",
    "            prediction += pd.Series(y_pred).rank().values  \n",
    "        \n",
    "        if model_type == 'lgb':\n",
    "            # feature importance\n",
    "            fold_importance = pd.DataFrame()\n",
    "            fold_importance[\"feature\"] = X.columns\n",
    "            fold_importance[\"importance\"] = model.feature_importances_\n",
    "            fold_importance[\"fold\"] = fold_n + 1\n",
    "            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n",
    "\n",
    "    prediction /= n_fold\n",
    "    \n",
    "    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n",
    "    \n",
    "    if model_type == 'lgb':\n",
    "        feature_importance[\"importance\"] /= n_fold\n",
    "        if plot_feature_importance:\n",
    "            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n",
    "                by=\"importance\", ascending=False)[:50].index\n",
    "\n",
    "            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n",
    "\n",
    "            plt.figure(figsize=(16, 12));\n",
    "            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n",
    "            plt.title('LGB Features (avg over folds)');\n",
    "        \n",
    "            return oof, prediction, feature_importance\n",
    "        return oof, prediction, scores\n",
    "    \n",
    "    else:\n",
    "        return oof, prediction, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.8035238095238096\n",
      "Best parameters: {'C': 0.115, 'class_weight': None, 'penalty': 'l1', 'solver': 'liblinear'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JoonH\\AppData\\Local\\conda\\conda\\envs\\TF\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "model = linear_model.LogisticRegression(max_iter = 10000)\n",
    "parameter_grid = {'solver': ['liblinear'],\n",
    "                  'penalty': ['l1'],\n",
    "                  'C': [0.1,0.105,0.11,0.115],\n",
    "                  'class_weight': ['balanced', None]\n",
    "                 }\n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid=parameter_grid, cv=folds, scoring='roc_auc', n_jobs=-1)\n",
    "grid_search.fit(x_train, y_train)\n",
    "print('Best score: {}'.format(grid_search.best_score_))\n",
    "print('Best parameters: {}'.format(grid_search.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV mean score: 0.7382, std: 0.0563.\n"
     ]
    }
   ],
   "source": [
    "model = linear_model.LogisticRegression(class_weight='balanced', penalty='l1', C=0.11, solver='liblinear')\n",
    "oof_lr, prediction_lr, scores = train_model(x_train, x_test, y_train, params=None, model_type='sklearn', model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200 samples, validate on 50 samples\n",
      "Epoch 1/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 115.6572 - acc: 0.6600 - val_loss: 91.7909 - val_acc: 0.5600\n",
      "Epoch 2/100\n",
      "200/200 [==============================] - 0s 444us/step - loss: 79.0282 - acc: 0.6600 - val_loss: 60.2557 - val_acc: 0.5600\n",
      "Epoch 3/100\n",
      "200/200 [==============================] - 0s 449us/step - loss: 51.5957 - acc: 0.6600 - val_loss: 39.1784 - val_acc: 0.5600\n",
      "Epoch 4/100\n",
      "200/200 [==============================] - 0s 434us/step - loss: 33.6039 - acc: 0.6600 - val_loss: 25.6903 - val_acc: 0.5600\n",
      "Epoch 5/100\n",
      "200/200 [==============================] - 0s 424us/step - loss: 22.1618 - acc: 0.6150 - val_loss: 17.2018 - val_acc: 0.5600\n",
      "Epoch 6/100\n",
      "200/200 [==============================] - 0s 414us/step - loss: 14.9500 - acc: 0.6600 - val_loss: 11.7965 - val_acc: 0.5600\n",
      "Epoch 7/100\n",
      "200/200 [==============================] - 0s 399us/step - loss: 10.3247 - acc: 0.6600 - val_loss: 8.2840 - val_acc: 0.5600\n",
      "Epoch 8/100\n",
      "200/200 [==============================] - 0s 399us/step - loss: 7.2970 - acc: 0.6600 - val_loss: 5.9504 - val_acc: 0.5600\n",
      "Epoch 9/100\n",
      "200/200 [==============================] - 0s 399us/step - loss: 5.2685 - acc: 0.6600 - val_loss: 4.3606 - val_acc: 0.5600\n",
      "Epoch 10/100\n",
      "200/200 [==============================] - 0s 399us/step - loss: 3.8741 - acc: 0.6600 - val_loss: 3.2489 - val_acc: 0.5600\n",
      "Epoch 11/100\n",
      "200/200 [==============================] - 0s 399us/step - loss: 2.8909 - acc: 0.6600 - val_loss: 2.4531 - val_acc: 0.5600\n",
      "Epoch 12/100\n",
      "200/200 [==============================] - 0s 394us/step - loss: 2.1820 - acc: 0.6600 - val_loss: 1.8724 - val_acc: 0.5600\n",
      "Epoch 13/100\n",
      "200/200 [==============================] - 0s 399us/step - loss: 1.6623 - acc: 0.6600 - val_loss: 1.4429 - val_acc: 0.5600\n",
      "Epoch 14/100\n",
      "200/200 [==============================] - 0s 399us/step - loss: 1.2767 - acc: 0.6600 - val_loss: 1.1221 - val_acc: 0.5600\n",
      "Epoch 15/100\n",
      "200/200 [==============================] - 0s 389us/step - loss: 0.9881 - acc: 0.6600 - val_loss: 0.8806 - val_acc: 0.5600\n",
      "Epoch 16/100\n",
      "200/200 [==============================] - 0s 399us/step - loss: 0.7703 - acc: 0.6600 - val_loss: 0.6963 - val_acc: 0.5600\n",
      "Epoch 17/100\n",
      "200/200 [==============================] - 0s 389us/step - loss: 0.6030 - acc: 0.6600 - val_loss: 0.5462 - val_acc: 0.5600\n",
      "Epoch 18/100\n",
      "200/200 [==============================] - 0s 394us/step - loss: 0.4641 - acc: 0.6600 - val_loss: 0.4073 - val_acc: 0.5600\n",
      "Epoch 19/100\n",
      "200/200 [==============================] - 0s 389us/step - loss: 0.3631 - acc: 0.6600 - val_loss: 0.3324 - val_acc: 0.5600\n",
      "Epoch 20/100\n",
      "200/200 [==============================] - 0s 389us/step - loss: 0.2937 - acc: 0.6600 - val_loss: 0.2867 - val_acc: 0.5600\n",
      "Epoch 21/100\n",
      "200/200 [==============================] - 0s 389us/step - loss: 0.2465 - acc: 0.6600 - val_loss: 0.2352 - val_acc: 0.5600\n",
      "Epoch 22/100\n",
      "200/200 [==============================] - 0s 389us/step - loss: 0.2060 - acc: 0.6600 - val_loss: 0.2007 - val_acc: 0.5600\n",
      "Epoch 23/100\n",
      "200/200 [==============================] - 0s 389us/step - loss: 0.1745 - acc: 0.6600 - val_loss: 0.1800 - val_acc: 0.5600\n",
      "Epoch 24/100\n",
      "200/200 [==============================] - 0s 394us/step - loss: 0.1534 - acc: 0.6600 - val_loss: 0.1561 - val_acc: 0.5600\n",
      "Epoch 25/100\n",
      "200/200 [==============================] - 0s 389us/step - loss: 0.1361 - acc: 0.6600 - val_loss: 0.1571 - val_acc: 0.5600\n",
      "Epoch 26/100\n",
      "200/200 [==============================] - 0s 404us/step - loss: 0.1260 - acc: 0.6600 - val_loss: 0.1356 - val_acc: 0.5600\n",
      "Epoch 27/100\n",
      "200/200 [==============================] - 0s 404us/step - loss: 0.1165 - acc: 0.6600 - val_loss: 0.1276 - val_acc: 0.5600\n",
      "Epoch 28/100\n",
      "200/200 [==============================] - 0s 399us/step - loss: 0.1076 - acc: 0.6600 - val_loss: 0.1288 - val_acc: 0.5600\n",
      "Epoch 29/100\n",
      "200/200 [==============================] - 0s 399us/step - loss: 0.1063 - acc: 0.6600 - val_loss: 0.1204 - val_acc: 0.5600\n",
      "Epoch 30/100\n",
      "200/200 [==============================] - 0s 399us/step - loss: 0.0990 - acc: 0.6600 - val_loss: 0.1158 - val_acc: 0.5600\n",
      "Epoch 31/100\n",
      "200/200 [==============================] - 0s 399us/step - loss: 0.0964 - acc: 0.6600 - val_loss: 0.1114 - val_acc: 0.5600\n",
      "Epoch 32/100\n",
      "200/200 [==============================] - 0s 399us/step - loss: 0.0952 - acc: 0.6600 - val_loss: 0.1091 - val_acc: 0.5600\n",
      "Epoch 33/100\n",
      "200/200 [==============================] - 0s 399us/step - loss: 0.0929 - acc: 0.6600 - val_loss: 0.1270 - val_acc: 0.5600\n",
      "Epoch 34/100\n",
      "200/200 [==============================] - 0s 399us/step - loss: 0.0990 - acc: 0.6600 - val_loss: 0.1046 - val_acc: 0.5600\n",
      "Epoch 35/100\n",
      "200/200 [==============================] - 0s 404us/step - loss: 0.0931 - acc: 0.6600 - val_loss: 0.1094 - val_acc: 0.5600\n",
      "Epoch 36/100\n",
      "200/200 [==============================] - 0s 404us/step - loss: 0.0951 - acc: 0.6600 - val_loss: 0.1199 - val_acc: 0.5600\n",
      "Epoch 37/100\n",
      "200/200 [==============================] - 0s 394us/step - loss: 0.0920 - acc: 0.6600 - val_loss: 0.1033 - val_acc: 0.5600\n",
      "Epoch 38/100\n",
      "200/200 [==============================] - 0s 389us/step - loss: 0.0891 - acc: 0.6600 - val_loss: 0.1119 - val_acc: 0.5600\n",
      "Epoch 39/100\n",
      "200/200 [==============================] - 0s 399us/step - loss: 0.0880 - acc: 0.6600 - val_loss: 0.1022 - val_acc: 0.5600\n",
      "Epoch 40/100\n",
      "200/200 [==============================] - 0s 394us/step - loss: 0.0906 - acc: 0.6600 - val_loss: 0.1169 - val_acc: 0.5600\n",
      "Epoch 41/100\n",
      "200/200 [==============================] - 0s 404us/step - loss: 0.0886 - acc: 0.6600 - val_loss: 0.1018 - val_acc: 0.5600\n",
      "Epoch 42/100\n",
      "200/200 [==============================] - 0s 394us/step - loss: 0.0880 - acc: 0.6600 - val_loss: 0.1090 - val_acc: 0.5600\n",
      "Epoch 43/100\n",
      "200/200 [==============================] - 0s 404us/step - loss: 0.0879 - acc: 0.6600 - val_loss: 0.1050 - val_acc: 0.5600\n",
      "Epoch 44/100\n",
      "200/200 [==============================] - 0s 394us/step - loss: 0.0869 - acc: 0.6600 - val_loss: 0.1037 - val_acc: 0.5600\n",
      "Epoch 45/100\n",
      "200/200 [==============================] - 0s 404us/step - loss: 0.0871 - acc: 0.6600 - val_loss: 0.1042 - val_acc: 0.5600\n",
      "Epoch 46/100\n",
      "200/200 [==============================] - 0s 394us/step - loss: 0.0867 - acc: 0.6600 - val_loss: 0.1066 - val_acc: 0.5600\n",
      "Epoch 47/100\n",
      "200/200 [==============================] - 0s 399us/step - loss: 0.0868 - acc: 0.6600 - val_loss: 0.1029 - val_acc: 0.5600\n",
      "Epoch 48/100\n",
      "200/200 [==============================] - 0s 394us/step - loss: 0.0901 - acc: 0.6600 - val_loss: 0.1040 - val_acc: 0.5600\n",
      "Epoch 49/100\n",
      "200/200 [==============================] - 0s 399us/step - loss: 0.0887 - acc: 0.6600 - val_loss: 0.1144 - val_acc: 0.5600\n",
      "Epoch 50/100\n",
      "200/200 [==============================] - 0s 394us/step - loss: 0.0887 - acc: 0.6600 - val_loss: 0.1042 - val_acc: 0.5600\n",
      "Epoch 51/100\n",
      "200/200 [==============================] - 0s 399us/step - loss: 0.0903 - acc: 0.6600 - val_loss: 0.1114 - val_acc: 0.5600\n",
      "Epoch 52/100\n",
      "200/200 [==============================] - 0s 399us/step - loss: 0.0896 - acc: 0.6600 - val_loss: 0.1021 - val_acc: 0.5600\n",
      "Epoch 53/100\n",
      "200/200 [==============================] - 0s 399us/step - loss: 0.0882 - acc: 0.6600 - val_loss: 0.1106 - val_acc: 0.5600\n",
      "Epoch 54/100\n",
      "200/200 [==============================] - 0s 399us/step - loss: 0.0873 - acc: 0.6600 - val_loss: 0.1038 - val_acc: 0.5600\n",
      "Epoch 55/100\n",
      "200/200 [==============================] - 0s 399us/step - loss: 0.0879 - acc: 0.6600 - val_loss: 0.1075 - val_acc: 0.5600\n",
      "Epoch 56/100\n",
      "200/200 [==============================] - 0s 404us/step - loss: 0.0885 - acc: 0.6600 - val_loss: 0.1060 - val_acc: 0.5600\n",
      "Epoch 57/100\n",
      "200/200 [==============================] - 0s 399us/step - loss: 0.0884 - acc: 0.6600 - val_loss: 0.1043 - val_acc: 0.5600\n",
      "Epoch 58/100\n",
      "200/200 [==============================] - 0s 394us/step - loss: 0.0876 - acc: 0.6600 - val_loss: 0.1031 - val_acc: 0.5600\n",
      "Epoch 59/100\n",
      "200/200 [==============================] - 0s 399us/step - loss: 0.0878 - acc: 0.6600 - val_loss: 0.1108 - val_acc: 0.5600\n",
      "Epoch 60/100\n",
      "200/200 [==============================] - 0s 399us/step - loss: 0.0874 - acc: 0.6600 - val_loss: 0.1032 - val_acc: 0.5600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/100\n",
      "200/200 [==============================] - 0s 394us/step - loss: 0.0879 - acc: 0.6600 - val_loss: 0.1062 - val_acc: 0.5600\n",
      "Epoch 62/100\n",
      "200/200 [==============================] - 0s 399us/step - loss: 0.0872 - acc: 0.6600 - val_loss: 0.1038 - val_acc: 0.5600\n",
      "Epoch 63/100\n",
      "200/200 [==============================] - 0s 394us/step - loss: 0.0872 - acc: 0.6600 - val_loss: 0.1061 - val_acc: 0.5600\n",
      "Epoch 64/100\n",
      "200/200 [==============================] - 0s 404us/step - loss: 0.0865 - acc: 0.6600 - val_loss: 0.1019 - val_acc: 0.5600\n",
      "Epoch 65/100\n",
      "200/200 [==============================] - 0s 394us/step - loss: 0.0876 - acc: 0.6600 - val_loss: 0.1054 - val_acc: 0.5600\n",
      "Epoch 66/100\n",
      "200/200 [==============================] - 0s 404us/step - loss: 0.0880 - acc: 0.6600 - val_loss: 0.1050 - val_acc: 0.5600\n",
      "Epoch 67/100\n",
      "200/200 [==============================] - 0s 409us/step - loss: 0.0872 - acc: 0.6600 - val_loss: 0.1071 - val_acc: 0.5600\n",
      "Epoch 68/100\n",
      "200/200 [==============================] - 0s 399us/step - loss: 0.0867 - acc: 0.6600 - val_loss: 0.1024 - val_acc: 0.5600\n",
      "Epoch 69/100\n",
      "200/200 [==============================] - 0s 394us/step - loss: 0.0877 - acc: 0.6600 - val_loss: 0.1055 - val_acc: 0.5600\n",
      "Epoch 70/100\n",
      "200/200 [==============================] - 0s 399us/step - loss: 0.0892 - acc: 0.6600 - val_loss: 0.1095 - val_acc: 0.5600\n",
      "Epoch 71/100\n",
      "200/200 [==============================] - 0s 394us/step - loss: 0.0868 - acc: 0.6600 - val_loss: 0.1031 - val_acc: 0.5600\n",
      "Epoch 72/100\n",
      "200/200 [==============================] - 0s 404us/step - loss: 0.0895 - acc: 0.6600 - val_loss: 0.1097 - val_acc: 0.5600\n",
      "Epoch 73/100\n",
      "200/200 [==============================] - 0s 399us/step - loss: 0.0929 - acc: 0.6600 - val_loss: 0.1252 - val_acc: 0.5600\n",
      "Epoch 74/100\n",
      "200/200 [==============================] - 0s 404us/step - loss: 0.0999 - acc: 0.6600 - val_loss: 0.1153 - val_acc: 0.5600\n",
      "Epoch 75/100\n",
      "200/200 [==============================] - 0s 404us/step - loss: 0.0884 - acc: 0.6600 - val_loss: 0.1168 - val_acc: 0.5600\n",
      "Epoch 76/100\n",
      "200/200 [==============================] - 0s 419us/step - loss: 0.0920 - acc: 0.6600 - val_loss: 0.1030 - val_acc: 0.5600\n",
      "Epoch 77/100\n",
      "200/200 [==============================] - 0s 409us/step - loss: 0.0909 - acc: 0.6600 - val_loss: 0.1048 - val_acc: 0.5600\n",
      "Epoch 78/100\n",
      "200/200 [==============================] - 0s 404us/step - loss: 0.0882 - acc: 0.6600 - val_loss: 0.1044 - val_acc: 0.5600\n",
      "Epoch 79/100\n",
      "200/200 [==============================] - 0s 404us/step - loss: 0.0898 - acc: 0.6600 - val_loss: 0.1053 - val_acc: 0.5600\n",
      "Epoch 80/100\n",
      "200/200 [==============================] - 0s 404us/step - loss: 0.0873 - acc: 0.6600 - val_loss: 0.1041 - val_acc: 0.5600\n",
      "Epoch 81/100\n",
      "200/200 [==============================] - 0s 409us/step - loss: 0.0900 - acc: 0.6600 - val_loss: 0.1074 - val_acc: 0.5600\n",
      "Epoch 82/100\n",
      "200/200 [==============================] - 0s 409us/step - loss: 0.0885 - acc: 0.6600 - val_loss: 0.1035 - val_acc: 0.5600\n",
      "Epoch 83/100\n",
      "200/200 [==============================] - 0s 414us/step - loss: 0.0883 - acc: 0.6600 - val_loss: 0.1103 - val_acc: 0.5600\n",
      "Epoch 84/100\n",
      "200/200 [==============================] - 0s 409us/step - loss: 0.0897 - acc: 0.6600 - val_loss: 0.1015 - val_acc: 0.5600\n",
      "Epoch 85/100\n",
      "200/200 [==============================] - 0s 399us/step - loss: 0.0871 - acc: 0.6600 - val_loss: 0.1114 - val_acc: 0.5600\n",
      "Epoch 86/100\n",
      "200/200 [==============================] - 0s 394us/step - loss: 0.0888 - acc: 0.6600 - val_loss: 0.1049 - val_acc: 0.5600\n",
      "Epoch 87/100\n",
      "200/200 [==============================] - 0s 399us/step - loss: 0.0870 - acc: 0.6600 - val_loss: 0.1074 - val_acc: 0.5600\n",
      "Epoch 88/100\n",
      "200/200 [==============================] - 0s 399us/step - loss: 0.0891 - acc: 0.6600 - val_loss: 0.1021 - val_acc: 0.5600\n",
      "Epoch 89/100\n",
      "200/200 [==============================] - 0s 399us/step - loss: 0.0891 - acc: 0.6600 - val_loss: 0.1076 - val_acc: 0.5600\n",
      "Epoch 90/100\n",
      "200/200 [==============================] - 0s 399us/step - loss: 0.0886 - acc: 0.6600 - val_loss: 0.1033 - val_acc: 0.5600\n",
      "Epoch 91/100\n",
      "200/200 [==============================] - 0s 404us/step - loss: 0.0875 - acc: 0.6600 - val_loss: 0.1082 - val_acc: 0.5600\n",
      "Epoch 92/100\n",
      "200/200 [==============================] - 0s 399us/step - loss: 0.0882 - acc: 0.6600 - val_loss: 0.1048 - val_acc: 0.5600\n",
      "Epoch 93/100\n",
      "200/200 [==============================] - 0s 399us/step - loss: 0.0867 - acc: 0.6600 - val_loss: 0.1026 - val_acc: 0.5600\n",
      "Epoch 94/100\n",
      "200/200 [==============================] - 0s 399us/step - loss: 0.0867 - acc: 0.6600 - val_loss: 0.1045 - val_acc: 0.5600\n",
      "Epoch 95/100\n",
      "200/200 [==============================] - 0s 404us/step - loss: 0.0868 - acc: 0.6600 - val_loss: 0.1050 - val_acc: 0.5600\n",
      "Epoch 96/100\n",
      "200/200 [==============================] - 0s 404us/step - loss: 0.0869 - acc: 0.6600 - val_loss: 0.1032 - val_acc: 0.5600\n",
      "Epoch 97/100\n",
      "200/200 [==============================] - 0s 404us/step - loss: 0.0870 - acc: 0.6600 - val_loss: 0.1058 - val_acc: 0.5600\n",
      "Epoch 98/100\n",
      "200/200 [==============================] - 0s 404us/step - loss: 0.0868 - acc: 0.6600 - val_loss: 0.1033 - val_acc: 0.5600\n",
      "Epoch 99/100\n",
      "200/200 [==============================] - 0s 404us/step - loss: 0.0869 - acc: 0.6600 - val_loss: 0.1046 - val_acc: 0.5600\n",
      "Epoch 100/100\n",
      "200/200 [==============================] - 0s 404us/step - loss: 0.0867 - acc: 0.6600 - val_loss: 0.1036 - val_acc: 0.5600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x135d0eb66d8>"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train_nn, y_train, batch_size = 50, epochs = 100, verbose = 1, validation_split = 0.2, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(model=eval_model, data=(x_test, y_test), args=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id = test_df[['id']]\n",
    "x_test = test_df.drop(['id'], axis = 1)\n",
    "x_test = x_test.astype('float32')\n",
    "x_test_nn = np.expand_dims(x_test, axis = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19750/19750 [==============================] - 3s 151us/step\n"
     ]
    }
   ],
   "source": [
    "results = model.predict(x_test_nn, batch_size=30, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19750"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_id.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>250</td>\n",
       "      <td>0.383895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>251</td>\n",
       "      <td>0.325390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>252</td>\n",
       "      <td>0.288592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>253</td>\n",
       "      <td>0.427592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>254</td>\n",
       "      <td>0.251649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>255</td>\n",
       "      <td>0.156945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>256</td>\n",
       "      <td>0.174616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>257</td>\n",
       "      <td>0.102915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>258</td>\n",
       "      <td>0.372323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>259</td>\n",
       "      <td>0.141676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>260</td>\n",
       "      <td>0.267965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>261</td>\n",
       "      <td>0.164429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>262</td>\n",
       "      <td>0.149956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>263</td>\n",
       "      <td>0.376085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>264</td>\n",
       "      <td>0.133752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>265</td>\n",
       "      <td>0.408002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>266</td>\n",
       "      <td>0.255576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>267</td>\n",
       "      <td>0.376614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>268</td>\n",
       "      <td>0.246400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>269</td>\n",
       "      <td>0.201657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>270</td>\n",
       "      <td>0.213363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>271</td>\n",
       "      <td>0.410182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>272</td>\n",
       "      <td>0.259782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>273</td>\n",
       "      <td>0.203014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>274</td>\n",
       "      <td>0.190488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>275</td>\n",
       "      <td>0.338249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>276</td>\n",
       "      <td>0.024487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>277</td>\n",
       "      <td>0.412969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>278</td>\n",
       "      <td>0.340757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>279</td>\n",
       "      <td>0.333858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19720</th>\n",
       "      <td>19970</td>\n",
       "      <td>0.117536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19721</th>\n",
       "      <td>19971</td>\n",
       "      <td>0.421727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19722</th>\n",
       "      <td>19972</td>\n",
       "      <td>0.295089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19723</th>\n",
       "      <td>19973</td>\n",
       "      <td>0.052865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19724</th>\n",
       "      <td>19974</td>\n",
       "      <td>0.370715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19725</th>\n",
       "      <td>19975</td>\n",
       "      <td>0.244055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19726</th>\n",
       "      <td>19976</td>\n",
       "      <td>0.415058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19727</th>\n",
       "      <td>19977</td>\n",
       "      <td>0.226331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19728</th>\n",
       "      <td>19978</td>\n",
       "      <td>0.323997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19729</th>\n",
       "      <td>19979</td>\n",
       "      <td>0.455775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19730</th>\n",
       "      <td>19980</td>\n",
       "      <td>0.102982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19731</th>\n",
       "      <td>19981</td>\n",
       "      <td>0.072391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19732</th>\n",
       "      <td>19982</td>\n",
       "      <td>0.344882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19733</th>\n",
       "      <td>19983</td>\n",
       "      <td>0.183040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19734</th>\n",
       "      <td>19984</td>\n",
       "      <td>0.367512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19735</th>\n",
       "      <td>19985</td>\n",
       "      <td>0.241584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19736</th>\n",
       "      <td>19986</td>\n",
       "      <td>0.382492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19737</th>\n",
       "      <td>19987</td>\n",
       "      <td>0.384511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19738</th>\n",
       "      <td>19988</td>\n",
       "      <td>0.379973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19739</th>\n",
       "      <td>19989</td>\n",
       "      <td>0.212492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19740</th>\n",
       "      <td>19990</td>\n",
       "      <td>0.248913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19741</th>\n",
       "      <td>19991</td>\n",
       "      <td>0.345092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19742</th>\n",
       "      <td>19992</td>\n",
       "      <td>0.268953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19743</th>\n",
       "      <td>19993</td>\n",
       "      <td>0.057385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19744</th>\n",
       "      <td>19994</td>\n",
       "      <td>0.427833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19745</th>\n",
       "      <td>19995</td>\n",
       "      <td>0.260305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19746</th>\n",
       "      <td>19996</td>\n",
       "      <td>0.475923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19747</th>\n",
       "      <td>19997</td>\n",
       "      <td>0.159894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19748</th>\n",
       "      <td>19998</td>\n",
       "      <td>0.436653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19749</th>\n",
       "      <td>19999</td>\n",
       "      <td>0.143951</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19750 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id    target\n",
       "0        250  0.383895\n",
       "1        251  0.325390\n",
       "2        252  0.288592\n",
       "3        253  0.427592\n",
       "4        254  0.251649\n",
       "5        255  0.156945\n",
       "6        256  0.174616\n",
       "7        257  0.102915\n",
       "8        258  0.372323\n",
       "9        259  0.141676\n",
       "10       260  0.267965\n",
       "11       261  0.164429\n",
       "12       262  0.149956\n",
       "13       263  0.376085\n",
       "14       264  0.133752\n",
       "15       265  0.408002\n",
       "16       266  0.255576\n",
       "17       267  0.376614\n",
       "18       268  0.246400\n",
       "19       269  0.201657\n",
       "20       270  0.213363\n",
       "21       271  0.410182\n",
       "22       272  0.259782\n",
       "23       273  0.203014\n",
       "24       274  0.190488\n",
       "25       275  0.338249\n",
       "26       276  0.024487\n",
       "27       277  0.412969\n",
       "28       278  0.340757\n",
       "29       279  0.333858\n",
       "...      ...       ...\n",
       "19720  19970  0.117536\n",
       "19721  19971  0.421727\n",
       "19722  19972  0.295089\n",
       "19723  19973  0.052865\n",
       "19724  19974  0.370715\n",
       "19725  19975  0.244055\n",
       "19726  19976  0.415058\n",
       "19727  19977  0.226331\n",
       "19728  19978  0.323997\n",
       "19729  19979  0.455775\n",
       "19730  19980  0.102982\n",
       "19731  19981  0.072391\n",
       "19732  19982  0.344882\n",
       "19733  19983  0.183040\n",
       "19734  19984  0.367512\n",
       "19735  19985  0.241584\n",
       "19736  19986  0.382492\n",
       "19737  19987  0.384511\n",
       "19738  19988  0.379973\n",
       "19739  19989  0.212492\n",
       "19740  19990  0.248913\n",
       "19741  19991  0.345092\n",
       "19742  19992  0.268953\n",
       "19743  19993  0.057385\n",
       "19744  19994  0.427833\n",
       "19745  19995  0.260305\n",
       "19746  19996  0.475923\n",
       "19747  19997  0.159894\n",
       "19748  19998  0.436653\n",
       "19749  19999  0.143951\n",
       "\n",
       "[19750 rows x 2 columns]"
      ]
     },
     "execution_count": 470,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = prediction_lr\n",
    "predictions = pd.DataFrame(results, columns = ['target'])\n",
    "\n",
    "ids = pd.DataFrame(test_id, columns = ['id'])\n",
    "predictions = pd.concat([ids, predictions], axis = 1, sort=False)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.to_csv('dont_overfit_2_logreg_bootstrap.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstrapping\n",
    "\n",
    "Use pseudo-labelled test data as our training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>2.165</td>\n",
       "      <td>0.681</td>\n",
       "      <td>-0.614</td>\n",
       "      <td>1.309</td>\n",
       "      <td>-0.455</td>\n",
       "      <td>-0.236</td>\n",
       "      <td>0.276</td>\n",
       "      <td>-2.246</td>\n",
       "      <td>...</td>\n",
       "      <td>0.867</td>\n",
       "      <td>1.347</td>\n",
       "      <td>0.504</td>\n",
       "      <td>-0.649</td>\n",
       "      <td>0.672</td>\n",
       "      <td>-2.097</td>\n",
       "      <td>1.051</td>\n",
       "      <td>-0.414</td>\n",
       "      <td>1.038</td>\n",
       "      <td>-1.065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1.081</td>\n",
       "      <td>-0.973</td>\n",
       "      <td>-0.383</td>\n",
       "      <td>0.326</td>\n",
       "      <td>-0.428</td>\n",
       "      <td>0.317</td>\n",
       "      <td>1.172</td>\n",
       "      <td>0.352</td>\n",
       "      <td>0.004</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.165</td>\n",
       "      <td>-1.695</td>\n",
       "      <td>-1.257</td>\n",
       "      <td>1.359</td>\n",
       "      <td>-0.808</td>\n",
       "      <td>-1.624</td>\n",
       "      <td>-0.458</td>\n",
       "      <td>-1.099</td>\n",
       "      <td>-0.936</td>\n",
       "      <td>0.973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.523</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>-0.348</td>\n",
       "      <td>0.148</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>0.404</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.172</td>\n",
       "      <td>0.137</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.263</td>\n",
       "      <td>-1.222</td>\n",
       "      <td>0.726</td>\n",
       "      <td>1.444</td>\n",
       "      <td>-1.165</td>\n",
       "      <td>-1.544</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.800</td>\n",
       "      <td>-1.211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.067</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.392</td>\n",
       "      <td>-1.637</td>\n",
       "      <td>-0.446</td>\n",
       "      <td>-0.725</td>\n",
       "      <td>-1.035</td>\n",
       "      <td>0.834</td>\n",
       "      <td>0.503</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>0.640</td>\n",
       "      <td>-0.595</td>\n",
       "      <td>-0.966</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.467</td>\n",
       "      <td>-0.562</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.533</td>\n",
       "      <td>0.238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2.347</td>\n",
       "      <td>-0.831</td>\n",
       "      <td>0.511</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>1.225</td>\n",
       "      <td>1.594</td>\n",
       "      <td>0.585</td>\n",
       "      <td>1.509</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>...</td>\n",
       "      <td>0.898</td>\n",
       "      <td>0.134</td>\n",
       "      <td>2.415</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-1.006</td>\n",
       "      <td>1.378</td>\n",
       "      <td>1.246</td>\n",
       "      <td>1.478</td>\n",
       "      <td>0.428</td>\n",
       "      <td>0.253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 301 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   target      0      1      2      3      4      5      6      7      8  \\\n",
       "0       1 -0.098  2.165  0.681 -0.614  1.309 -0.455 -0.236  0.276 -2.246   \n",
       "1       0  1.081 -0.973 -0.383  0.326 -0.428  0.317  1.172  0.352  0.004   \n",
       "2       1 -0.523 -0.089 -0.348  0.148 -0.022  0.404 -0.023 -0.172  0.137   \n",
       "3       1  0.067 -0.021  0.392 -1.637 -0.446 -0.725 -1.035  0.834  0.503   \n",
       "4       1  2.347 -0.831  0.511 -0.021  1.225  1.594  0.585  1.509 -0.012   \n",
       "\n",
       "   ...      290    291    292    293    294    295    296    297    298    299  \n",
       "0  ...    0.867  1.347  0.504 -0.649  0.672 -2.097  1.051 -0.414  1.038 -1.065  \n",
       "1  ...   -0.165 -1.695 -1.257  1.359 -0.808 -1.624 -0.458 -1.099 -0.936  0.973  \n",
       "2  ...    0.013  0.263 -1.222  0.726  1.444 -1.165 -1.544  0.004  0.800 -1.211  \n",
       "3  ...   -0.404  0.640 -0.595 -0.966  0.900  0.467 -0.562 -0.254 -0.533  0.238  \n",
       "4  ...    0.898  0.134  2.415 -0.996 -1.006  1.378  1.246  1.478  0.428  0.253  \n",
       "\n",
       "[5 rows x 301 columns]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_df = pd.read_csv('/Users/JoonH/dont-overfit-ii/bootstrap_data.csv', nrows = 254)\n",
    "p_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(254, 300)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = p_df.drop(['target'], axis = 1)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(254, 300)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-08 17:14:59,612 featuretools.entityset - WARNING    index id not found in dataframe, creating new integer column\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Entityset: test\n",
       "  Entities:\n",
       "    test [Rows: 19750, Columns: 301]\n",
       "  Relationships:\n",
       "    No relationships"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import featuretools as ft\n",
    "\n",
    "# initialize entityset\n",
    "es = ft.EntitySet('data')\n",
    "es2 = ft.EntitySet('test')\n",
    "\n",
    "# add entities (application table itself)\n",
    "es.entity_from_dataframe(\n",
    "    entity_id='main', # define entity id\n",
    "    dataframe=p_df.drop(['target'], axis=1), # select underlying data\n",
    "    index='id', # define unique index column\n",
    "    # specify some datatypes manually (if needed)\n",
    "    variable_types={\n",
    "        f: ft.variable_types.Categorical \n",
    "        for f in train_df.columns if f.startswith('FLAG_')\n",
    "    }\n",
    ")\n",
    "\n",
    "es2.entity_from_dataframe(\n",
    "    entity_id='test', # define entity id\n",
    "    dataframe=test_df, # select underlying data\n",
    "    index='id', # define unique index column\n",
    "    # specify some datatypes manually (if needed)\n",
    "    variable_types={\n",
    "        f: ft.variable_types.Categorical \n",
    "        for f in train_df.columns if f.startswith('FLAG_')\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built 600 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JoonH\\AppData\\Local\\conda\\conda\\envs\\TF\\lib\\site-packages\\featuretools\\computational_backends\\utils.py:107: UserWarning: Chunk size is greater than size of feature matrix\n",
      "  warnings.warn(\"Chunk size is greater than size of feature matrix\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 00:00 | Remaining: 00:00 | Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| Calculated: 1/1 chunks\n",
      "Built 600 features\n",
      "Elapsed: 00:02 | Remaining: 00:00 | Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| Calculated: 2/2 chunks\n"
     ]
    }
   ],
   "source": [
    "#The actual feature construction\n",
    "# see feature set definitions (no actual computations yet)\n",
    "# used for faster prototyping\n",
    "fm_train,feature_defs = ft.dfs(\n",
    "    entityset=es, \n",
    "    target_entity=\"main\", \n",
    "    features_only=False,\n",
    "    agg_primitives=[\n",
    "        \"mean\",\n",
    "        \"mode\", \n",
    "        \"max\", \n",
    "        \"min\", \n",
    "        \"sum\", \n",
    "        \"std\"\n",
    "        \n",
    "    ],\n",
    "    trans_primitives=[\n",
    "        \"not\",\n",
    "        \"diff\",\n",
    "        \"not\",\n",
    "        \"percentile\",\n",
    "        \"cum_sum\"\n",
    "    ],\n",
    "    max_depth=2,\n",
    "    #cutoff_time=cutoff_times,\n",
    "    #training_window=ft.Timedelta(60, \"d\"), # use only last X days in computations\n",
    "    max_features=1000,\n",
    "    chunk_size=5000,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "fm_test,feature_defs = ft.dfs(\n",
    "    entityset=es2, \n",
    "    target_entity=\"test\", \n",
    "    features_only=False,\n",
    "    agg_primitives=[\n",
    "        \"mean\",\n",
    "        \"mode\", \n",
    "        \"max\", \n",
    "        \"min\", \n",
    "        \"sum\", \n",
    "        \"std\"\n",
    "        \n",
    "    ],\n",
    "    trans_primitives=[\n",
    "        \"not\",\n",
    "        \"diff\",\n",
    "        \"not\",\n",
    "        \"percentile\",\n",
    "        \"cum_sum\"\n",
    "    ],\n",
    "    max_depth=2,\n",
    "    #cutoff_time=cutoff_times,\n",
    "    #training_window=ft.Timedelta(60, \"d\"), # use only last X days in computations\n",
    "    max_features=1000,\n",
    "    chunk_size=10000,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#folds = StratifiedKFold(n_splits=25, shuffle=True, random_state=42)\n",
    "folds = KFold(n_splits=25, shuffle=True, random_state=42)\n",
    "repeated_folds = RepeatedStratifiedKFold(n_splits=25, n_repeats=20, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "y_train = p_df['target']\n",
    "#x_train = fm_train\n",
    "x_train = p_df.drop(['target'], axis = 1)\n",
    "x_test = test_df.drop(['id'], axis = 1)\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "#x_test = fm_test\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV mean score: 0.6778, std: 0.2795.\n"
     ]
    }
   ],
   "source": [
    "model = linear_model.LogisticRegression(class_weight='balanced', penalty='l2', C=0.09, solver='liblinear', max_iter = 100000)\n",
    "oof_lr, prediction_lr, scores = train_model(x_train, x_test, y_train, params=None, model_type='sklearn', model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_train = fm_train.drop_duplicates()\n",
    "fm_test = fm_test.drop_duplicates()\n",
    "\n",
    "from boruta import BorutaPy\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(n_estimators = 100, n_jobs = -1, class_weight = 'balanced')\n",
    "boruta_selector = BorutaPy(rfc, n_estimators = 'auto', verbose = 0)\n",
    "boruta_selector.fit(x_train,y_train)\n",
    "\n",
    "feature_df = pd.DataFrame(fm_train.columns.tolist(),columns = ['features'])\n",
    "feature_df['rank'] = boruta_selector.ranking_\n",
    "feature_df = feature_df.sort_values('rank',ascending=True).reset_index(drop=True)\n",
    "columns_to_keep = feature_df.features[0:400]\n",
    "boruta_train = fm_train[columns_to_keep]\n",
    "boruta_test = fm_test[columns_to_keep]\n",
    "\n",
    "n_fold = 80\n",
    "folds = StratifiedKFold(n_splits=250, shuffle=True, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(boruta_train)\n",
    "X_test = scaler.transform(boruta_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV mean score: 0.7655, std: 0.1368.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "model = linear_model.LogisticRegression(class_weight='balanced', penalty='l2', C=0.15, solver='liblinear', max_iter = 100000)\n",
    "oof_lr, prediction_lr, scores = train_model(X_train, X_test, y_train, params=None, model_type='sklearn', model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eli5.sklearn import PermutationImportance\n",
    "perm = PermutationImportance(model, random_state=1).fit(X_train, y_train)\n",
    "#eli5.show_weights(perm, top=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "sel = SelectFromModel(perm,threshold=0.001, prefit=True)\n",
    "X_trans = sel.transform(x_train)\n",
    "X_test_trans = sel.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV mean score: 0.6186, std: 0.1403.\n"
     ]
    }
   ],
   "source": [
    "model = linear_model.LogisticRegression(class_weight='balanced', penalty='l1', C=0.17, solver='liblinear', max_iter = 50000)\n",
    "oof_lr, prediction_lr, _ = train_model(X_trans, X_test_trans, y_train, params=None, model_type='sklearn', model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV mean score: 0.8026, std: 0.1070.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc = SVC(C = 8.0, kernel='rbf', probability = True, gamma = 'auto')\n",
    "oof_lr_svm, prediction_lr_svm, _ = train_model(X_trans, X_test_trans, y_train, params=None, model_type='sklearn', model=svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results = prediction_lr_svm * 0.3 + prediction_lr * 0.7\n",
    "results = prediction_lr\n",
    "predictions = pd.DataFrame(results, columns = ['target'])\n",
    "\n",
    "ids = test_df['id']\n",
    "predictions = pd.concat([ids, predictions], axis = 1, sort=False)\n",
    "predictions.to_csv('dont_overfit_2_bootstrap2.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
