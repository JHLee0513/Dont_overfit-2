{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CapsNet Approach\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras import initializers, layers\n",
    "import numpy as np\n",
    "from keras import layers, models, optimizers\n",
    "from keras import backend as K\n",
    "from keras.utils import to_categorical\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import TensorBoard\n",
    "import gc\n",
    "gc.enable()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"/Users/JoonH/dont-overfit-ii/train.csv\")\n",
    "test_df = pd.read_csv(\"/Users/JoonH/dont-overfit-ii/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_df.drop(['id','target'],axis=1)\n",
    "y_train = train_df['target']\n",
    "x_test = test_df.drop(['id'],axis=1)\n",
    "test_ids = test_df['id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting features\n",
    "\n",
    "Since a lot of the features are often seen not that important by other simpler algorithms, we are going to try to run a subset of features from the original dataset. The following set of list was developed from running Recursive Feature Elimination CV with logistic regression model. Though one model's important features do not necessarily guarentee important features for another, this is probably a good-enough baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>280</th>\n",
       "      <th>279</th>\n",
       "      <th>278</th>\n",
       "      <th>277</th>\n",
       "      <th>276</th>\n",
       "      <th>275</th>\n",
       "      <th>274</th>\n",
       "      <th>273</th>\n",
       "      <th>272</th>\n",
       "      <th>271</th>\n",
       "      <th>...</th>\n",
       "      <th>10</th>\n",
       "      <th>9</th>\n",
       "      <th>8</th>\n",
       "      <th>7</th>\n",
       "      <th>16</th>\n",
       "      <th>18</th>\n",
       "      <th>20</th>\n",
       "      <th>22</th>\n",
       "      <th>24</th>\n",
       "      <th>26</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.617</td>\n",
       "      <td>0.484</td>\n",
       "      <td>-1.125</td>\n",
       "      <td>-1.049</td>\n",
       "      <td>0.820</td>\n",
       "      <td>0.156</td>\n",
       "      <td>-1.120</td>\n",
       "      <td>-2.145</td>\n",
       "      <td>-1.516</td>\n",
       "      <td>-1.320</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.912</td>\n",
       "      <td>1.825</td>\n",
       "      <td>-2.246</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.177</td>\n",
       "      <td>-0.503</td>\n",
       "      <td>0.410</td>\n",
       "      <td>0.102</td>\n",
       "      <td>1.763</td>\n",
       "      <td>-1.097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.043</td>\n",
       "      <td>2.012</td>\n",
       "      <td>1.215</td>\n",
       "      <td>0.422</td>\n",
       "      <td>2.064</td>\n",
       "      <td>0.932</td>\n",
       "      <td>0.491</td>\n",
       "      <td>1.312</td>\n",
       "      <td>-0.516</td>\n",
       "      <td>-2.771</td>\n",
       "      <td>...</td>\n",
       "      <td>2.907</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.352</td>\n",
       "      <td>1.098</td>\n",
       "      <td>-0.498</td>\n",
       "      <td>-1.100</td>\n",
       "      <td>1.382</td>\n",
       "      <td>-1.519</td>\n",
       "      <td>-0.128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.045</td>\n",
       "      <td>2.535</td>\n",
       "      <td>-0.302</td>\n",
       "      <td>-0.730</td>\n",
       "      <td>-0.208</td>\n",
       "      <td>-1.122</td>\n",
       "      <td>-0.527</td>\n",
       "      <td>0.780</td>\n",
       "      <td>0.346</td>\n",
       "      <td>-0.727</td>\n",
       "      <td>...</td>\n",
       "      <td>0.459</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.137</td>\n",
       "      <td>-0.172</td>\n",
       "      <td>-1.044</td>\n",
       "      <td>-1.038</td>\n",
       "      <td>-1.658</td>\n",
       "      <td>0.633</td>\n",
       "      <td>1.786</td>\n",
       "      <td>-0.103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.864</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.597</td>\n",
       "      <td>-1.701</td>\n",
       "      <td>1.527</td>\n",
       "      <td>-1.226</td>\n",
       "      <td>0.845</td>\n",
       "      <td>0.176</td>\n",
       "      <td>2.725</td>\n",
       "      <td>-0.710</td>\n",
       "      <td>...</td>\n",
       "      <td>0.335</td>\n",
       "      <td>0.274</td>\n",
       "      <td>0.503</td>\n",
       "      <td>0.834</td>\n",
       "      <td>0.210</td>\n",
       "      <td>-0.326</td>\n",
       "      <td>-0.764</td>\n",
       "      <td>-1.308</td>\n",
       "      <td>0.365</td>\n",
       "      <td>-0.808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.830</td>\n",
       "      <td>-0.883</td>\n",
       "      <td>0.190</td>\n",
       "      <td>-0.692</td>\n",
       "      <td>-0.523</td>\n",
       "      <td>0.325</td>\n",
       "      <td>-1.073</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.464</td>\n",
       "      <td>0.055</td>\n",
       "      <td>...</td>\n",
       "      <td>0.190</td>\n",
       "      <td>2.198</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>1.509</td>\n",
       "      <td>-1.312</td>\n",
       "      <td>-0.688</td>\n",
       "      <td>-0.285</td>\n",
       "      <td>-0.315</td>\n",
       "      <td>0.024</td>\n",
       "      <td>1.656</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     280    279    278    277    276    275    274    273    272    271  \\\n",
       "0  0.617  0.484 -1.125 -1.049  0.820  0.156 -1.120 -2.145 -1.516 -1.320   \n",
       "1  0.043  2.012  1.215  0.422  2.064  0.932  0.491  1.312 -0.516 -2.771   \n",
       "2 -1.045  2.535 -0.302 -0.730 -0.208 -1.122 -0.527  0.780  0.346 -0.727   \n",
       "3  1.864  0.150  0.597 -1.701  1.527 -1.226  0.845  0.176  2.725 -0.710   \n",
       "4 -1.830 -0.883  0.190 -0.692 -0.523  0.325 -1.073  0.014 -0.464  0.055   \n",
       "\n",
       "   ...       10      9      8      7     16     18     20     22     24     26  \n",
       "0  ...   -0.912  1.825 -2.246  0.276  0.177 -0.503  0.410  0.102  1.763 -1.097  \n",
       "1  ...    2.907 -0.291  0.004  0.352  1.098 -0.498 -1.100  1.382 -1.519 -0.128  \n",
       "2  ...    0.459  0.183  0.137 -0.172 -1.044 -1.038 -1.658  0.633  1.786 -0.103  \n",
       "3  ...    0.335  0.274  0.503  0.834  0.210 -0.326 -0.764 -1.308  0.365 -0.808  \n",
       "4  ...    0.190  2.198 -0.012  1.509 -1.312 -0.688 -0.285 -0.315  0.024  1.656  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top 30 feature rankings from RFECV\n",
    "columns_to_keep = ['280','279','278','277','276','275','274','273',\n",
    "                   '272','271','270','269','268','267','266','265',\n",
    "                   '1','15','14','12','10','9','8','7','16','18','20',\n",
    "                   '22','24','26']\n",
    "x_train = x_train[columns_to_keep]\n",
    "x_test = x_test[columns_to_keep]\n",
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = RobustScaler().fit_transform(np.concatenate((x_train, x_test), axis=0))\n",
    "train = data[:250]\n",
    "test = data[250:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250, 30)\n",
      "(250, 30, 1) (19750, 30, 1)\n"
     ]
    }
   ],
   "source": [
    "#now scaled, expand their dimensions to fit into NN\n",
    "print(train.shape)\n",
    "train = train.astype('float32')\n",
    "test = test.astype('float32')\n",
    "train = np.expand_dims(train, axis = -1)\n",
    "test = np.expand_dims(test, axis = -1)\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our model\n",
    "\n",
    "Our model is derived from \n",
    "* https://github.com/XifengGuo/CapsNet-Fashion-MNIST\n",
    "* https://keras.io/examples/cifar10_cnn_capsule/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Length(layers.Layer):\n",
    "    \"\"\"\n",
    "    Compute the length of vectors. This is used to compute a Tensor that has the same shape with y_true in margin_loss.\n",
    "    Using this layer as model's output can directly predict labels by using `y_pred = np.argmax(model.predict(x), 1)`\n",
    "    inputs: shape=[None, num_vectors, dim_vector]\n",
    "    output: shape=[None, num_vectors]\n",
    "    \"\"\"\n",
    "    def call(self, inputs, **kwargs):\n",
    "        return K.sqrt(K.sum(K.square(inputs), -1))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[:-1]\n",
    "\n",
    "\n",
    "class Mask(layers.Layer):\n",
    "    \"\"\"\n",
    "    Mask a Tensor with shape=[None, num_capsule, dim_vector] either by the capsule with max length or by an additional \n",
    "    input mask. Except the max-length capsule (or specified capsule), all vectors are masked to zeros. Then flatten the\n",
    "    masked Tensor.\n",
    "    For example:\n",
    "        ```\n",
    "        x = keras.layers.Input(shape=[8, 3, 2])  # batch_size=8, each sample contains 3 capsules with dim_vector=2\n",
    "        y = keras.layers.Input(shape=[8, 3])  # True labels. 8 samples, 3 classes, one-hot coding.\n",
    "        out = Mask()(x)  # out.shape=[8, 6]\n",
    "        # or\n",
    "        out2 = Mask()([x, y])  # out2.shape=[8,6]. Masked with true labels y. Of course y can also be manipulated.\n",
    "        ```\n",
    "    \"\"\"\n",
    "    def call(self, inputs, **kwargs):\n",
    "        if type(inputs) is list:  # true label is provided with shape = [None, n_classes], i.e. one-hot code.\n",
    "            assert len(inputs) == 2\n",
    "            inputs, mask = inputs\n",
    "        else:  # if no true label, mask by the max length of capsules. Mainly used for prediction\n",
    "            # compute lengths of capsules\n",
    "            x = K.sqrt(K.sum(K.square(inputs), -1))\n",
    "            # generate the mask which is a one-hot code.\n",
    "            # mask.shape=[None, n_classes]=[None, num_capsule]\n",
    "            mask = K.one_hot(indices=K.argmax(x, 1), num_classes=x.get_shape().as_list()[1])\n",
    "\n",
    "        # inputs.shape=[None, num_capsule, dim_capsule]\n",
    "        # mask.shape=[None, num_capsule]\n",
    "        # masked.shape=[None, num_capsule * dim_capsule]\n",
    "        masked = K.batch_flatten(inputs * K.expand_dims(mask, -1))\n",
    "        return masked\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if type(input_shape[0]) is tuple:  # true label provided\n",
    "            return tuple([None, input_shape[0][1] * input_shape[0][2]])\n",
    "        else:  # no true label provided\n",
    "            return tuple([None, input_shape[1] * input_shape[2]])\n",
    "\n",
    "\n",
    "def squash(vectors, axis=-1):\n",
    "    \"\"\"\n",
    "    The non-linear activation used in Capsule. It drives the length of a large vector to near 1 and small vector to 0\n",
    "    :param vectors: some vectors to be squashed, N-dim tensor\n",
    "    :param axis: the axis to squash\n",
    "    :return: a Tensor with same shape as input vectors\n",
    "    \"\"\"\n",
    "    s_squared_norm = K.sum(K.square(vectors), axis, keepdims=True)\n",
    "    scale = s_squared_norm / (1 + s_squared_norm) / K.sqrt(s_squared_norm + K.epsilon())\n",
    "    return scale * vectors\n",
    "\n",
    "\n",
    "class CapsuleLayer(layers.Layer):\n",
    "    \"\"\"\n",
    "    The capsule layer. It is similar to Dense layer. Dense layer has `in_num` inputs, each is a scalar, the output of the \n",
    "    neuron from the former layer, and it has `out_num` output neurons. CapsuleLayer just expand the output of the neuron\n",
    "    from scalar to vector. So its input shape = [None, input_num_capsule, input_dim_capsule] and output shape = \\\n",
    "    [None, num_capsule, dim_capsule]. For Dense Layer, input_dim_capsule = dim_capsule = 1.\n",
    "    \n",
    "    :param num_capsule: number of capsules in this layer\n",
    "    :param dim_capsule: dimension of the output vectors of the capsules in this layer\n",
    "    :param num_routing: number of iterations for the routing algorithm\n",
    "    \"\"\"\n",
    "    def __init__(self, num_capsule, dim_capsule, num_routing=3,\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 **kwargs):\n",
    "        super(CapsuleLayer, self).__init__(**kwargs)\n",
    "        self.num_capsule = num_capsule\n",
    "        self.dim_capsule = dim_capsule\n",
    "        self.num_routing = num_routing\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) >= 3, \"The input Tensor should have shape=[None, input_num_capsule, input_dim_capsule]\"\n",
    "        self.input_num_capsule = input_shape[1]\n",
    "        self.input_dim_capsule = input_shape[2]\n",
    "\n",
    "        # Transform matrix\n",
    "        self.W = self.add_weight(shape=[self.num_capsule, self.input_num_capsule,\n",
    "                                        self.dim_capsule, self.input_dim_capsule],\n",
    "                                 initializer=self.kernel_initializer,\n",
    "                                 name='W')\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        # inputs.shape=[None, input_num_capsule, input_dim_capsule]\n",
    "        # inputs_expand.shape=[None, 1, input_num_capsule, input_dim_capsule]\n",
    "        inputs_expand = K.expand_dims(inputs, 1)\n",
    "\n",
    "        # Replicate num_capsule dimension to prepare being multiplied by W\n",
    "        # inputs_tiled.shape=[None, num_capsule, input_num_capsule, input_dim_capsule]\n",
    "        inputs_tiled = K.tile(inputs_expand, [1, self.num_capsule, 1, 1])\n",
    "\n",
    "        # Compute `inputs * W` by scanning inputs_tiled on dimension 0.\n",
    "        # x.shape=[num_capsule, input_num_capsule, input_dim_capsule]\n",
    "        # W.shape=[num_capsule, input_num_capsule, dim_capsule, input_dim_capsule]\n",
    "        # Regard the first two dimensions as `batch` dimension,\n",
    "        # then matmul: [input_dim_capsule] x [dim_capsule, input_dim_capsule]^T -> [dim_capsule].\n",
    "        # inputs_hat.shape = [None, num_capsule, input_num_capsule, dim_capsule]\n",
    "        inputs_hat = K.map_fn(lambda x: K.batch_dot(x, self.W, [2, 3]), elems=inputs_tiled)\n",
    "\n",
    "        \"\"\"\n",
    "        # Begin: routing algorithm V1, dynamic ------------------------------------------------------------#\n",
    "        # The prior for coupling coefficient, initialized as zeros.\n",
    "        b = K.zeros(shape=[self.batch_size, self.num_capsule, self.input_num_capsule])\n",
    "        def body(i, b, outputs):\n",
    "            c = tf.nn.softmax(b, dim=1)  # dim=2 is the num_capsule dimension\n",
    "            outputs = squash(K.batch_dot(c, inputs_hat, [2, 2]))\n",
    "            if i != 1:\n",
    "                b = b + K.batch_dot(outputs, inputs_hat, [2, 3])\n",
    "            return [i-1, b, outputs]\n",
    "        cond = lambda i, b, inputs_hat: i > 0\n",
    "        loop_vars = [K.constant(self.num_routing), b, K.sum(inputs_hat, 2, keepdims=False)]\n",
    "        shape_invariants = [tf.TensorShape([]),\n",
    "                            tf.TensorShape([None, self.num_capsule, self.input_num_capsule]),\n",
    "                            tf.TensorShape([None, self.num_capsule, self.dim_capsule])]\n",
    "        _, _, outputs = tf.while_loop(cond, body, loop_vars, shape_invariants)\n",
    "        # End: routing algorithm V1, dynamic ------------------------------------------------------------#\n",
    "        \"\"\"\n",
    "        # Begin: Routing algorithm ---------------------------------------------------------------------#\n",
    "        # In forward pass, `inputs_hat_stopped` = `inputs_hat`;\n",
    "        # In backward, no gradient can flow from `inputs_hat_stopped` back to `inputs_hat`.\n",
    "        inputs_hat_stopped = K.stop_gradient(inputs_hat)\n",
    "        \n",
    "        # The prior for coupling coefficient, initialized as zeros.\n",
    "        # b.shape = [None, self.num_capsule, self.input_num_capsule].\n",
    "        b = tf.zeros(shape=[K.shape(inputs_hat)[0], self.num_capsule, self.input_num_capsule])\n",
    "\n",
    "        assert self.num_routing > 0, 'The num_routing should be > 0.'\n",
    "        for i in range(self.num_routing):\n",
    "            # c.shape=[batch_size, num_capsule, input_num_capsule]\n",
    "            c = tf.nn.softmax(b, dim=1)\n",
    "\n",
    "            # At last iteration, use `inputs_hat` to compute `outputs` in order to backpropagate gradient\n",
    "            if i == self.num_routing - 1:\n",
    "                # c.shape =  [batch_size, num_capsule, input_num_capsule]\n",
    "                # inputs_hat.shape=[None, num_capsule, input_num_capsule, dim_capsule]\n",
    "                # The first two dimensions as `batch` dimension,\n",
    "                # then matmal: [input_num_capsule] x [input_num_capsule, dim_capsule] -> [dim_capsule].\n",
    "                # outputs.shape=[None, num_capsule, dim_capsule]\n",
    "                outputs = squash(K.batch_dot(c, inputs_hat, [2, 2]))  # [None, 10, 16]\n",
    "            else:  # Otherwise, use `inputs_hat_stopped` to update `b`. No gradients flow on this path.\n",
    "                outputs = squash(K.batch_dot(c, inputs_hat_stopped, [2, 2]))\n",
    "\n",
    "                # outputs.shape =  [None, num_capsule, dim_capsule]\n",
    "                # inputs_hat.shape=[None, num_capsule, input_num_capsule, dim_capsule]\n",
    "                # The first two dimensions as `batch` dimension,\n",
    "                # then matmal: [dim_capsule] x [input_num_capsule, dim_capsule]^T -> [input_num_capsule].\n",
    "                # b.shape=[batch_size, num_capsule, input_num_capsule]\n",
    "                b += K.batch_dot(outputs, inputs_hat_stopped, [2, 3])\n",
    "        # End: Routing algorithm -----------------------------------------------------------------------#\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return tuple([None, self.num_capsule, self.dim_capsule])\n",
    "\n",
    "\n",
    "def PrimaryCap(inputs, dim_capsule, n_channels, kernel_size, strides, padding):\n",
    "    \"\"\"\n",
    "    Apply Conv2D `n_channels` times and concatenate all capsules\n",
    "    :param inputs: 4D tensor, shape=[None, width, height, channels]\n",
    "    :param dim_capsule: the dim of the output vector of capsule\n",
    "    :param n_channels: the number of types of capsules\n",
    "    :return: output tensor, shape=[None, num_capsule, dim_capsule]\n",
    "    \"\"\"\n",
    "    output = layers.Conv2D(filters=dim_capsule*n_channels, kernel_size=kernel_size, strides=strides, padding=padding,\n",
    "                           name='primarycap_conv2d')(inputs)\n",
    "    outputs = layers.Reshape(target_shape=[-1, dim_capsule], name='primarycap_reshape')(output)\n",
    "    return layers.Lambda(squash, name='primarycap_squash')(outputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.set_image_data_format('channels_last')\n",
    "\n",
    "def CapsNet(input_shape, n_class, num_routing):\n",
    "    \"\"\"\n",
    "    A Capsule Network on MNIST.\n",
    "    :param input_shape: data shape, 3d, [width, height, channels]\n",
    "    :param n_class: number of classes\n",
    "    :param num_routing: number of routing iterations\n",
    "    :return: Two Keras Models, the first one used for training, and the second one for evaluation.\n",
    "            `eval_model` can also be used for training.\n",
    "    \"\"\"\n",
    "    x = layers.Input(shape=input_shape)\n",
    "\n",
    "    conv1 = layers.Conv1D(filters=128, kernel_size=9, strides=1, padding='valid', activation='relu', name='conv1')(x)\n",
    "    conv2 = layers.Conv1D(filters=64, kernel_size=9, strides=2, padding='valid', activation='relu', name='conv2')(conv1)\n",
    "\n",
    "    outputs = layers.Reshape(target_shape=[-1, 8], name='primarycap_reshape')(conv2)\n",
    "    primarycaps = layers.Lambda(squash, name='primarycap_squash')(outputs)\n",
    "\n",
    "    # Layer 3: Capsule layer. Routing algorithm works here.\n",
    "    digitcaps = CapsuleLayer(num_capsule=n_class, dim_capsule=16, num_routing=num_routing,\n",
    "                             name='digitcaps')(primarycaps)\n",
    "\n",
    "    # Layer 4: This is an auxiliary layer to replace each capsule with its length. Just to match the true label's shape.\n",
    "    # If using tensorflow, this will not be necessary. :)\n",
    "    out_caps = Length(name='capsnet')(digitcaps)\n",
    "\n",
    "    # Decoder network.\n",
    "    y = layers.Input(shape=(n_class,))\n",
    "    masked_by_y = Mask()([digitcaps, y])  # The true label is used to mask the output of capsule layer. For training\n",
    "    masked = Mask()(digitcaps)  # Mask using the capsule with maximal length. For prediction\n",
    "\n",
    "    # Shared Decoder model in training and prediction\n",
    "    decoder = models.Sequential(name='decoder')\n",
    "    decoder.add(layers.Dense(64, activation='relu', input_dim=16*n_class))\n",
    "    decoder.add(layers.Dense(128, activation='relu'))\n",
    "    decoder.add(layers.Dense(np.prod(input_shape), activation='sigmoid'))\n",
    "    decoder.add(layers.Reshape(target_shape=input_shape, name='out_recon'))\n",
    "\n",
    "    # Models for training and evaluation (prediction)\n",
    "    train_model = models.Model([x, y], [out_caps, decoder(masked_by_y)])\n",
    "    eval_model = models.Model(x, [out_caps, decoder(masked)])\n",
    "    return train_model, eval_model\n",
    "\n",
    "\n",
    "def margin_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Margin loss for Eq.(4). When y_true[i, :] contains not just one `1`, this loss should work too. Not test it.\n",
    "    :param y_true: [None, n_classes]\n",
    "    :param y_pred: [None, num_capsule]\n",
    "    :return: a scalar loss value.\n",
    "    \"\"\"\n",
    "    L = y_true * K.square(K.maximum(0., 0.9 - y_pred)) + \\\n",
    "        0.5 * (1 - y_true) * K.square(K.maximum(0., y_pred - 0.1))\n",
    "\n",
    "    return K.mean(K.sum(L, 1))\n",
    "\n",
    "\n",
    "def train(model, data, save_dir,lr,lam_recon, batch_size,epochs):\n",
    "    \"\"\"\n",
    "    Training a CapsuleNet\n",
    "    :param model: the CapsuleNet model\n",
    "    :param data: a tuple containing training and testing data, like `((x_train, y_train), (x_test, y_test))`\n",
    "    :param args: arguments\n",
    "    :return: The trained model\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    # callbacks\n",
    "    log = callbacks.CSVLogger(save_dir + '/log.csv')\n",
    "    #tb = callbacks.TensorBoard(log_dir=args.save_dir + '/tensorboard-logs',\n",
    "    #                           batch_size=args.batch_size, histogram_freq=args.debug)\n",
    "    checkpoint = callbacks.ModelCheckpoint(save_dir + '/weights-{epoch:02d}.h5', monitor='val_capsnet_acc',\n",
    "                                           save_best_only=True, save_weights_only=True, verbose=1)\n",
    "    #lr_decay = callbacks.LearningRateScheduler(schedule=lambda epoch: args.lr * (0.95 ** epoch))\n",
    "\n",
    "    # compile the model\n",
    "    model.compile(optimizer=optimizers.Adamax(lr=lr),\n",
    "                  loss=[margin_loss, 'mse'],\n",
    "                  loss_weights=[1., lam_recon],\n",
    "                  metrics={'capsnet': 'accuracy'})\n",
    "\n",
    "    \"\"\"\n",
    "    # Training without data augmentation:\n",
    "    model.fit([x_train, y_train], [y_train, x_train], batch_size=args.batch_size, epochs=args.epochs,\n",
    "              validation_data=[[x_test, y_test], [y_test, x_test]], callbacks=[log, tb, checkpoint, lr_decay])\n",
    "    \"\"\"\n",
    "\n",
    "    # Begin: Training with data augmentation ---------------------------------------------------------------------#\n",
    "    def train_generator(x, y, batch_size, shift_fraction=0.):\n",
    "        train_datagen = ImageDataGenerator(width_shift_range=shift_fraction,\n",
    "                                           height_shift_range=shift_fraction,\n",
    "                                           horizontal_flip=False)  # shift up to 2 pixel for MNIST\n",
    "        generator = train_datagen.flow(x, y, batch_size=batch_size)\n",
    "        while 1:\n",
    "            x_batch, y_batch = generator.next()\n",
    "            yield ([x_batch, y_batch], [y_batch, x_batch])\n",
    "\n",
    "    # Training with data augmentation. If shift_fraction=0., also no augmentation.\n",
    "    model.fit_generator(generator=train_generator(x_train, y_train, batch_size),\n",
    "                        steps_per_epoch=int(y_train.shape[0] / batch_size),\n",
    "                        epochs=epochs,\n",
    "                        validation_data=[[x_test, y_test], [y_test, x_test]],\n",
    "                        callbacks=[log, checkpoint])\n",
    "    # End: Training with data augmentation -----------------------------------------------------------------------#\n",
    "\n",
    "    model.save_weights(save_dir + '/trained_model.h5')\n",
    "    print('Trained model saved to \\'%s/trained_model.h5\\'' % save_dir)\n",
    "\n",
    "    from utils import plot_log\n",
    "    plot_log(save_dir + '/log.csv', show=True)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def test(model, data):\n",
    "    x_test, y_test = data\n",
    "    y_pred, x_recon = model.predict(x_test, batch_size=100)\n",
    "    print('-'*50)\n",
    "    print('Test acc:', np.sum(np.argmax(y_pred, 1) == np.argmax(y_test, 1))/y_test.shape[0])\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    from utils import combine_images\n",
    "    from PIL import Image\n",
    "\n",
    "    img = combine_images(np.concatenate([x_test[:50],x_recon[:50]]))\n",
    "    image = img * 255\n",
    "    Image.fromarray(image.astype(np.uint8)).save(\"real_and_recon.png\")\n",
    "    print()\n",
    "    print('Reconstructed images are saved to ./real_and_recon.png')\n",
    "    print('-'*50)\n",
    "    plt.imshow(plt.imread(\"real_and_recon.png\", ))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    x_train = train_df.drop(['target','id'],axis=1)\n",
    "    \n",
    "    y_train = train_df['target']\n",
    "    x_test = test_df.drop(['id'],axis=1)    \n",
    "    x_train = np.expand_dims(x_train, axis = -1)\n",
    "    x_train = np.array(x_train)\n",
    "    y_train = np.array(y_train)\n",
    "\n",
    "    #y_train = to_categorical(y_train.astype('float32'))\n",
    "    return (x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 300, 1)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv1D)                  (None, 292, 128)     1280        input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2 (Conv1D)                  (None, 142, 64)      73792       conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_reshape (Reshape)    (None, 1136, 8)      0           conv2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_squash (Lambda)      (None, 1136, 8)      0           primarycap_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "digitcaps (CapsuleLayer)        (None, 1, 16)        145408      primarycap_squash[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "input_12 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "mask_9 (Mask)                   (None, 16)           0           digitcaps[0][0]                  \n",
      "                                                                 input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "capsnet (Length)                (None, 1)            0           digitcaps[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Sequential)            (None, 300, 1)       48108       mask_9[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 268,588\n",
      "Trainable params: 268,588\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import callbacks\n",
    "\n",
    "# setting the hyper parameters\n",
    "\n",
    "batchsize = 100\n",
    "epochs = 100\n",
    "lam_recon = 0.392\n",
    "num_routing = 3\n",
    "shift_fraction = 0.\n",
    "debug = 0\n",
    "save_dir = \"/Users/JoonH/dont-overfit-ii/CapsNet/\"\n",
    "is_training = 1\n",
    "weights = None\n",
    "lr = 0.001\n",
    "        \n",
    "# load data\n",
    "(x_train, y_train) = load_data()\n",
    "# define model\n",
    "model, eval_model = CapsNet(input_shape=x_train.shape[1:],\n",
    "                            n_class=1,\n",
    "                            num_routing=num_routing)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Fri Mar 15 14:56:04 2019\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "('Input data in `NumpyArrayIterator` should have rank 4. You passed an array with shape', (201, 300, 1))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-51fa9a550717>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_valid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvalid_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_valid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvalid_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_dir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlam_recon\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatchsize\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# as long as weights are given, will run testing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mweights\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-38-10e3b3d1d492>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, data, save_dir, lr, lam_recon, batch_size, epochs)\u001b[0m\n\u001b[0;32m    103\u001b[0m                         \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m                         \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m                         callbacks=[log, checkpoint])\n\u001b[0m\u001b[0;32m    106\u001b[0m     \u001b[1;31m# End: Training with data augmentation -----------------------------------------------------------------------#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\TF\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\TF\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1418\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1420\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\TF\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    179\u001b[0m             \u001b[0mbatch_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 181\u001b[1;33m                 \u001b[0mgenerator_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    182\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'__len__'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\TF\\lib\\site-packages\\keras\\utils\\data_utils.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    707\u001b[0m                     \u001b[1;34m\"`use_multiprocessing=False, workers > 1`.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    708\u001b[0m                     \"For more information see issue #1638.\")\n\u001b[1;32m--> 709\u001b[1;33m             \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\TF\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mreraise\u001b[1;34m(tp, value, tb)\u001b[0m\n\u001b[0;32m    691\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    692\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 693\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    694\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    695\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\TF\\lib\\site-packages\\keras\\utils\\data_utils.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    683\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    684\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 685\u001b[1;33m                 \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    686\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0minputs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\TF\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    668\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    669\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 670\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    671\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    672\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\TF\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mworker\u001b[1;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[0mjob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mwrap_exception\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfunc\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0m_helper_reraises_exception\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\TF\\lib\\site-packages\\keras\\utils\\data_utils.py\u001b[0m in \u001b[0;36mnext_sample\u001b[1;34m(uid)\u001b[0m\n\u001b[0;32m    624\u001b[0m         \u001b[0mThe\u001b[0m \u001b[0mnext\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0mof\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0muid\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    625\u001b[0m     \"\"\"\n\u001b[1;32m--> 626\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_SHARED_SEQUENCES\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0muid\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    627\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-38-10e3b3d1d492>\u001b[0m in \u001b[0;36mtrain_generator\u001b[1;34m(x, y, batch_size, shift_fraction)\u001b[0m\n\u001b[0;32m     93\u001b[0m                                            \u001b[0mheight_shift_range\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshift_fraction\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m                                            horizontal_flip=False)  # shift up to 2 pixel for MNIST\n\u001b[1;32m---> 95\u001b[1;33m         \u001b[0mgenerator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_datagen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m             \u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\TF\\lib\\site-packages\\keras_preprocessing\\image.py\u001b[0m in \u001b[0;36mflow\u001b[1;34m(self, x, y, batch_size, shuffle, sample_weight, seed, save_to_dir, save_prefix, save_format, subset)\u001b[0m\n\u001b[0;32m    914\u001b[0m             \u001b[0msave_prefix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msave_prefix\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    915\u001b[0m             \u001b[0msave_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msave_format\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 916\u001b[1;33m             subset=subset)\n\u001b[0m\u001b[0;32m    917\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    918\u001b[0m     def flow_from_directory(self, directory,\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\TF\\lib\\site-packages\\keras_preprocessing\\image.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, image_data_generator, batch_size, shuffle, sample_weight, seed, data_format, save_to_dir, save_prefix, save_format, subset, dtype)\u001b[0m\n\u001b[0;32m   1631\u001b[0m             raise ValueError('Input data in `NumpyArrayIterator` '\n\u001b[0;32m   1632\u001b[0m                              \u001b[1;34m'should have rank 4. You passed an array '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1633\u001b[1;33m                              'with shape', self.x.shape)\n\u001b[0m\u001b[0;32m   1634\u001b[0m         \u001b[0mchannels_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdata_format\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'channels_last'\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1635\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mchannels_axis\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: ('Input data in `NumpyArrayIterator` should have rank 4. You passed an array with shape', (201, 300, 1))"
     ]
    }
   ],
   "source": [
    "# train or test\n",
    "import time\n",
    "if weights is not None:  # init the model weights with provided one\n",
    "    model.load_weights(args.weights)\n",
    "if is_training==1:\n",
    "    n_fold = 20\n",
    "    folds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=42)\n",
    "    for fold_n, (train_index, valid_index) in enumerate(folds.split(x_train, y_train)):\n",
    "        print('Fold', fold_n, 'started at', time.ctime())\n",
    "        x_train, x_valid = x_train[train_index], x_train[valid_index]\n",
    "        y_train, y_valid = y_train[train_index], y_train[valid_index]\n",
    "        train(model, ((x_train, y_train), (x_valid, y_valid)), save_dir,lr,lam_recon,batchsize,epochs)\n",
    "else:  # as long as weights are given, will run testing\n",
    "    if weights is None:\n",
    "        print('No weights are provided. Will test using random initialized weights.')\n",
    "    test(model=eval_model, data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
