{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial validation approach\n",
    "\n",
    "Here we explore adversarial validation to see if the usual cross-validation strategy will work. Many thanks to this kernel:\n",
    "* https://www.kaggle.com/konradb/adversarial-validation-and-other-scary-terms\n",
    "\n",
    "Also thanks to FastML: \n",
    "* http://fastml.com/adversarial-validation-part-one/\n",
    "* http://fastml.com/adversarial-validation-part-two/\n",
    "\n",
    "## To Summarize what Ad. Val. is..\n",
    "***\"The general idea is to check the degree of similarity between training and tests in terms of feature distribution: if they are difficult to distinguish, the distribution is probably similar and the usual validation techniques should work. It does not seem to be the case, so we can suspect they are quite different. This intuition can be quantified by combining train and test sets, assigning 0/1 labels (0 - train, 1-test) and evaluating a binary classification task.\"***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold, cross_val_score, GridSearchCV, RepeatedStratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import json\n",
    "import ast\n",
    "import time\n",
    "from sklearn import linear_model\n",
    "import eli5\n",
    "import gc\n",
    "gc.enable()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"/Users/JoonH/dont-overfit-ii/train.csv\")\n",
    "test_df = pd.read_csv(\"/Users/JoonH/dont-overfit-ii/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_df.drop(['target','id'],axis=1)\n",
    "id_train = train_df['id']\n",
    "x_test = test_df.drop(['id'],axis=1)\n",
    "id_test = test_df['id']\n",
    "\n",
    "x_train['is_test'] = 0\n",
    "x_test['is_test'] = 1\n",
    "\n",
    "x_combined = pd.concat([x_train,x_test],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x_combined['is_test']\n",
    "x_combined = x_combined.drop(['is_test'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fold = 4\n",
    "folds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(class_weight='balanced', penalty='l1', C=0.1, solver='liblinear', max_iter = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14999, 300)\n",
      "0.515091901483153\n",
      "(14999, 300)\n",
      "0.526715397918314\n",
      "(15001, 300)\n",
      "0.48282880422353924\n",
      "(15001, 300)\n",
      "0.5226662397825506\n"
     ]
    }
   ],
   "source": [
    "for train_index, test_index in folds.split(x_combined, y):\n",
    "        x0, x1 = x_combined.iloc[train_index], x_combined.iloc[test_index]\n",
    "        y0, y1 = y.iloc[train_index], y.iloc[test_index]        \n",
    "        print(x0.shape)\n",
    "        clf.fit(x0, y0)\n",
    "                \n",
    "        prval = clf.predict_proba(x1)[:,1]\n",
    "        print(roc_auc_score(y1,prval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quite Interestingly, the technique seems to show us that it is difficult for logistic regression, our currently best working model, fails to distinguish between the training dataset and test dataset. This tells us the two are not that easy to distinguish, thus indicating that the distribution is very similar and our usual validation score should work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to utilize Adversarial CV\n",
    "\n",
    "To implement this method given that the distribution was actually easy to distinguish, we would do the following:\n",
    "1. Train a model to predict whether given data belong to test set\n",
    "2. Predict train data to select subset that is closest to the test set\n",
    "3. Utilize that subset as our main cv data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
